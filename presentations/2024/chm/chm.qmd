---
title: 'Developing a "Smart" Recovery Monitoring and Support System'
author: "John J. Curtin, Ph.D."
institute: "University of Wisconsin-Madison"
date: October 2, 2024
format: 
  revealjs:
    scrollable: true
    css: chm.css
    slide-number: false
title-slide-attributes:
  data-background-image: https://github.com/jjcurtin/lectures/blob/main/images/smartphone_know_you.png?raw=true
  data-background-size: 35%
  data-background-repeat: no
  data-background-position: left 10% bottom 10%
include-after: |
  <script type="text/javascript">
    Reveal.on('ready', event => {
      if (event.indexh === 0) {
        document.querySelector("div.has-logo > img.slide-logo").style.display = "block";
      if (event.indexh === 0) {
        document.querySelector("div.has-logo > img.slide-logo").style.display = "block";
      }
        Reveal.configure({ slideNumber: null });
        document.querySelector("div.has-logo > img.slide-logo").style.display = "none";
        Reveal.configure({ slideNumber: 'c' });
editor_options: 
  chunk_output_type: console
---

## Mental Healthcare Needs are High and Unmet 

- In 2019, [52 million]{style="color: orange;"} Americans had an active mental illness
  - More than half did not receive any treatment


::: {.notes}
We have a mental health crisis in the U.S. and it is a crisis of **unmet** high need because our delivery of mental healthcare is deeply flawed. 

In 2019, **more than half of the 52 million Americans** with an active mental illness did not receive any treatment. 

**More than half**!
:::

-----------------------------------------------------------------------------

## Mental Healthcare Needs are High and Unmet

- [In 2019, 52 million Americans had an active mental illness]{style="color: grey;"}
  - [More than half did not receive any treatment]{style="color: grey;"}


- [20 million]{style="color: orange;"} adults had an active substance use disorder
  - 9 out of 10 did not receive any treatment


::: {.notes}
And for the 20 million adults suffering with a substance use disorder, it was worse still.

**9 out of 10 without any treatment**
:::

-----------------------------------------------------------------------------

## Mental Healthcare Needs are High and Unmet

- [In 2019, 52 million Americans had an active mental illness]{style="color: grey;"}
  - [More than half did not receive any treatment]{style="color: grey;"}

- [20 million adults had an active substance use disorder]{style="color: grey;"}
  - [9 out of 10 did not receive any treatment]{style="color: grey;"}

- [Large treatment disparities]{style="color: orange;"} exist by race, ethnicity, geography, and income


::: {.notes}
Our failure to treat is even more troubling for vulnerable groups.  Black and LatinX adults receive mental healthcare services at only half the rate of whites.  

And similar mental healthcare disparities exist for people living in rural communities and for those with lower incomes.
:::

-----------------------------------------------------------------------------

## Mental Healthcare Needs are High and Unmet

- Failure to treat is not surprising given many treatment barriers:
  - Access
  - Availability
  - Affordability
  - Acceptability


::: {.notes}
Our failure to treat is, unfortunately, not surprising.  There are many well known barriers to receiving traditional mental healthcare.

These include problems with access that that affect everyone but are particularly limiting for people living in rural communities

Problems with availability

Treatment costs are often prohibitive for those without health insurance 

and stigma and related issues make traditional treatments for mental illness less acceptable to some patients.  
:::

-----------------------------------------------------------------------------

## Digital Therapeutics (DTx)

- Smartphone "apps" that prevent, manage, or treat disease 

- Can augment mental health services to address barriers
  - Accessible everywhere
  - Available 24/7
  - Highly scalable (affordable?)

::: {.notes}
Fortunately, digital therapeutics are now being developed and used by patients to address many of these treatment barriers.

For those of you that are not yet familiar with this treatment modality, digital therapeutics are smartphone "apps" that are designed to prevent, manage, or treat disease, including substance use disorders and other mental illness. 

These digital therapeutics can be combined with traditional treatments to reduce barriers because they are

- Accessible everywhere

- Available everyday, 24/7

- and highlighly scalable, which may lower costs
:::

-----------------------------------------------------------------------------

## Digital Therapeutics (DTx)

- Smartphone "apps" that prevent, manage, or treat disease 

- Can augment mental health services to address barriers
  - [Accessible everywhere]{style="color: gray;"} 
  - [Available 24/7]{style="color: gray;"} 
  - [Highly scalable (affordable?)]{style="color: gray;"} 
  - Effective!

::: {.notes}
Of course, these benefits would be meaningless if digital therapeutics were not effective.

But they are.  

For example, patients with substance use disorders who use a digital therapeutic have almost double the odds of being abstinent from alcohol or other drugs

These increases in abstinence from using digital therapeutics are observed not only when compared to patients on wait lists, who have yet to gain access to treatment, but also when digital therapeutics are added on top of traditional treatments for substance use disorders.

And these benefits are durable - they have been documented up to 12 months after the start of treatment.  
:::

-----------------------------------------------------------------------------

## Digital Therapeutics (DTx)

- Smartphone "apps" that prevent, manage, or treat disease 

- Can augment mental health services to address barriers
  - [Accessible everywhere]{style="color: gray;"} 
  - [Available 24/7]{style="color: gray;"} 
  - [Highly scalable (affordable?)]{style="color: gray;"} 
  - Effective!

::: {.notes}
[PAUSE]

But let me pause for a moment.  I want to be very clear on one point before we move forward.  I do not believe or hope that digital therapeutics will replace human therapists. 
  
Therapists will always be needed for what they do uniquely well.  We simply need more than they can provide alone.  Digital therapeutics can provide that "more."
:::

-----------------------------------------------------------------------------

## {#beta_app data-menu-title="Image of beta app" background-image="http://dionysus.psych.wisc.edu/present/beta_app.png" background-size="100%" background-repeat="none"}


::: {.notes}
[PAUSE]

So as you likely know, digital therapeutics are in use today with patients with SUD, but I think these apps are still best considered beta versions relative to their full potential. 

Their power comes from easy, 24/7 access to their many supports - the treatments, tools, and services built into these smartphone apps. 

But this is also their Achilles heel. As the patient using these apps, you now have to tackle difficult questions like:

- When should I use them?

- For how long?

- Which of their many supports are best for me?

- And which are best for me **right now**, at this moment in time?
:::

-----------------------------------------------------------------------------

## Precision Mental Health w/ Smart DTx

- Precision mental health requires us to provide the [right treatments]{style="color: orange;"} and supports to the [right people]{style="color: orange;"} at the [right time]{style="color: orange;"}, [every time]{style="color: orange;"}

- Current DTx can already help with "every time" given reduced barriers to care

- Smart DTx can address "who", "when" & "why"

  - Sensing
  - AI/Machine learning

::: {.notes}
These are essentially precision mental health questions about providing the **right interventions and supports** to the **right people** at the **right time**, **every time**

Current digital therapeutics can already address the **every time** need given their 24/7 availability and reduced barriers to use

But to fully embrace this precision mental health goal, the next wave of digital therapeutics, lets call them **smart digital therapeutics**, must learn to know us well enough to recognize when we are at greatest risk for relapse and they must be smart enough to recommend the specific supports both inside and outside the app that would be most effective for us at that moment in time to prevent that relapse.

And we believe digital therapeutics can provide this **personalized care** if we enhance them with sensing capabilities and built-in artificial intelligence or machine learning risk prediction algorithms.
:::

-----------------------------------------------------------------------------

## Model Output: Lapses 

Lapses

- are clearly defined,
- have a temporally precise onset, and
- can serve as an early warning sign for relapse (precede and predict)

\

- "Abstinence violation effects" can increase relapse risk
- Even a single lapse can result in overdose and/or death for some drugs

::: {.notes}
[PAUSE]

OK - so how are we approaching this precision mental health challenge?

To start, we have begun to develop risk prediction models that focus both on **predicting** and **explaining** future lapses

Our focus is on future lapses rather than other clinically meaningful outcomes like substance use related problems or relapse for several reasons.

To start, lapses are

- Clearly defined and have a temporally precise onset
- They can serve as an early warning sign for relapse because they both precede and predict it

- Lapses are also important targets for intervention because we know that maladaptive thoughts and feelings following a lapse - often called abstinence violation effects - can start a downward spiral that leads to relapse by itself if not addressed   

- And sadly, for some drugs, even a single lapse can result in overdose and death


So for these reasons, we are developing risk models that predict the probability of a future lapse
:::

-----------------------------------------------------------------------------

## Model Inputs: Personal Sensing

Sensing requires:

- In Vivo measurement
- Longitudinal
- High temporal granularity 
- Feasible/acceptable for long term use

::: {.notes}
The next logical set of questions surround the model inputs.   

What will we use as inputs or risk features in these models to predict lapses?

And how will we measure the raw signals associated with these risk features?

We believe that personal sensing will play a big role here.  And when I talk about sensing, I am talking about a measurement strategy that allows for in vivo measurement within the context of an individual's day-to-day life.   

It must be longitudinal but also with high temporal granularity so that we have the temporal precision to intervene during days and even potentially moments of heightend risk

And it must be acceptable to people for long term use because we know that SUDs are chronic, relapsing disorders that require lifetime management and continuing care.
:::

-----------------------------------------------------------------------------

## Model Inputs: Personal Sensing

Sensing requires:

- [In Vivo measurement]{style="color: gray;"}
- [Longitudinal]{style="color: gray;"}
- [High temporal granularity]{style="color: gray;"}
- [Feasible/acceptable for long term use]{style="color: gray;"}

\

- Consistent across platforms/devices
- Stable/Low churn

::: {.notes}
There are also some practical requirements.  

If we are going to make the sensing and and prediction system widely available, then the measurement must be feasible, consistent, and stable across common equipment that most people own.  

From our perspective, this limits us to smartphones with the major operating systems
:::

-----------------------------------------------------------------------------

## Model Inputs: Personal Sensing

- Ecological Momentary Assessments (EMA)
- Contextualized Geolocation
- Contexualized smartphone communications

::: {.notes}
Given these sensing requirements, we are focusing the majority of our effort at this point on feature engineering with three separate but complementary sets of raw signals that can be sensed easily on any smartphone. These are

- Ecological momentary assessments
- Geolocation
- Cellular communications meta data and text message content

Our goal is to use features from these powerful signals to develop lapse prediction models with exceptionally high temporal resolution

In other words, we aren’t looking to simply identify individuals at high risk for lapses

Instead we want to know precisely when these lapses will occur so that we can intervene to prevent them
:::

-----------------------------------------------------------------------------

## Lapse Prediction for AUD

::: {.columns}
:::: {.column width="60%"}
- 151 individuals with moderate to severe AUD
- Early in recovery (1-8 weeks)
- Committed to abstinence throughout study
- Followed with sensing for up to 3 months
  - [Ecological Momentary Assessments]{style="color: orange;"}
  - Contextualized Geolocation
  - Contexualized smartphone communications
  - (also sensed physiology, sleep, coarse self-report)
::::

:::: {.column width="40%"}
![risk1_pis.png](https://github.com/jjcurtin/lectures/blob/main/images/risk1_pis.png?raw=true)\    
![niaaa_logo.png](https://github.com/jjcurtin/lectures/blob/main/images/niaaa_logo.png?raw=true)\    
::::
:::

::: {.notes}
So let me transition now to describing how we are taking the first baby steps toward developing smart digital therapeutics for SUD

We have recently completed a NIAAA funded project where we collected data from 151 participants who were in early recovery from a moderate to severe alcohol use disorder.  

These participants were committed to abstinence at the start of the study and we followed them for up to 3 months, collecting a variety of personal sensing data streams including the three I just metioned.

Our broad goal was to develop a sensing and prediction system that could be embedded in any digital therapeutic to both predict future lapses and to understand the risk factors contributing to those predicted lapses so that we can provide appropriate support in advance

And critically, this system must perform well enough to be useful when applied to individuals.  

In other words, we are not looking to find coarse group-level predictors of lapses but instead to make predictions for specfic individuals at specific moments in time that are actionable
:::

-----------------------------------------------------------------------------

## Lapse Prediction for AUD

::: {.columns}
:::: {.column width="60%"}
- 151 individuals with moderate to severe AUD
- Early in recovery (1-8 weeks)
- Committed to abstinence throughout study
- Followed with sensing for up to 3 months
  - [Ecological Momentary Assessments]{style="color: orange;"}
  - Contextualized Geolocation
  - Contexualized smartphone communications
  - (also sensed physiology, sleep, coarse self-report)
::::

:::: {.column width="40%"}
![risk1_pis.png](https://github.com/jjcurtin/lectures/blob/main/images/risk1_pis.png?raw=true)\    
![niaaa_logo.png](https://github.com/jjcurtin/lectures/blob/main/images/niaaa_logo.png?raw=true)\    
::::
:::

::: {.notes}
[PAUSE]

We are in the early stages of model building at this point and I will focus today primarily on results from preliminary models using only EMA.  

However, we are actively working with GPS and cellular communications as well and I will give you a clear sense of how we are developing models with those signals too.  

I'll also give you more detail about how we think we can implement these models for clinical benefits.  

And if there is time, I'll end by contrasting this work with other precision mental health models we are developing to use outside of digital therapeutics
:::

-----------------------------------------------------------------------------

## Participant Characteristics

```{r}
#| label: figs_ema_demographics
#| fig-height: 6
#| fig-width: 10

source(here::here("figs/risk/figs_ema_demographics.R"))
cowplot::plot_grid(fig_age, fig_sex, fig_race, fig_ms, fig_educ, fig_income, nrow = 2, ncol = 3)
```

::: {.notes}
Let me start highlighting the characteristics of the sample we are using to develop and evaluate the AUD lapse models.

We had reasonble diversity across many participant charactersitics including age, sex at birth, martital status, education, and income

However, given the methods we used for recruiting, we have very little racial and ethinic diversity in the sample.  The sample is predominately white and non-hispanic

I'll return to this later both when we evaluate issues of algorithmic fairness and when I talk about another, larger NIDA funded project where we are correcting this issue of racial and ethnic representation in our models.  
:::

-----------------------------------------------------------------------------

## Participant Characteristics

::: {.columns}
:::: {.column width="50%"}
- All participants met criteria for moderate to severe AUD
- Reported abstinence goals
::::

:::: {.column width="50%"}
```{r}
#| label: figs_sx
#| fig-height: 6
#| fig-width: 6 

# plot made in fig_demographics.R
fig_sx
```
::::
:::

::: {.notes}
I also want to highlight that this is a sample with clinically meaningful problems with their AUD use, consistent with their pursuit of abstinence goals

On the right, I am showing you a histogram of the DSM-5 AUD symptom counts to confirm that everyone reported 4 or more symptoms of alcohol use disorder consistent with a moderate to severe presentation
:::

-----------------------------------------------------------------------------

## Ecological Momentary Assessments 
 
::: {.columns}
:::: {.column width="60%"}
![risk1_ema_questions.png](https://github.com/jjcurtin/lectures/blob/main/images/risk1_ema_questions.png?raw=true){.absolute width="60%" top="15%" height="auto"}\    
::::

:::: {.column width="30%"}
- Current
  - Craving
  - Affect
  - Risky situations
  - Stressful events
  - Pleasant events

- Future
  - Risky situations
  - Stressful events
  - Confidence
::::
:::

::: {.notes}
So let me tell you a bit more about the 4x daily EMA we collected for three months during their study participation  

On each EMA, participants reported the date and time of any lapses back to alcohol use that they hadn’t previously reported. These lapse reports are used for the lapse outcomes that we train our models to predict.  And these laspes were also confirmed by study staff during lab visits using a follow-back procedure.

All of the EMAs also asked participants about their current craving, affective valence and arousal, recent risky situations, and recent stressful and pleasant events since their last EMA.

And on the first EMA each day, they also reported any future risky situations and stressful events that they expected in the next week and their confidence that they would remain abstinent. 
:::

-----------------------------------------------------------------------------

## Modeling: Predictions 

- Predict hour-by-hour probability of [future lapse]{style="color: orange;"}

- Lapse window widths
  - 1 week 
  - 1 day
  - 1 hour 

::: {.notes}
OK, now that we've talked about sensing, I want to transition to training models to use features from these raw signals to predict lapses.

For our purposes today I wont dive deep into the machine learning methods but let me highlight a few high level details 

We used features from these sensed signals that I just described to make predictions about the hour-by-hour probability of a future lapse.  We are developing separate models for three future lapse windows – lapses in the next week, lapses in the next day, and lapses in the next hour.  

For example, if I was in recovery from an AUD, I could use these models to generate the probability that I would lapse after this symposium starting at 1 pm.  One model would generate the probability I would lapse at some point between 1 pm today and 1 pm next tuesday, the second would predict the probability of a lapse between 1 pm today and 1 pm tomorrow and the third and most temporally precise model would provide the probability of a lapse in the next hour, between 1pm today and 2 pm today.  

And of course, all of the models would only use data collected prior to 1 pm today so that they are “predicting”, in the full sense of the word, into the future and not just demonstrating an association.
:::

-----------------------------------------------------------------------------

## Modeling: Algorithms and Resampling

- XGBoost - Boosted decision trees
- Also considered:
  - ElasticNet GLM (e.g., LASSO, ridge regression)
  - Random Forest
  - KNN

\

- Using grouped (by participant), nested, repeated k-fold CV 
  - 30 "held-out" test sets
  - New participants and observations not used for training

::: {.notes}
We are evaluating machine learning model configurations that differ by common statistical algorithms that I can talk more about later if there is interest.

And we are rigorously evaluating the performance of these models using grouped, nested, repeated k-fold cross validation.  

For our purposes, what this means is that we evalute model performance in 30 separate held out test sets and each of these sets contain new observatations from new participants that were not used to train the models.  

And again, this is consistent with what we mean by prediction.  We don't care how our models perform with the participants we used to train them. We want to know how well the models will work when we implement them with new people in the future.
:::

-----------------------------------------------------------------------------

## Modeling: Feature Engineering 

- Features based on recent past experiences (12, 24, 48, 72, 168 hours)

- Min, max, and median response (all items)

- History (count) of past lapses (item 1) and completed EMAs (compliance)

- Raw scores and change scores (from baseline/all past responses)

::: {.notes}
We used the raw responses to the EMAs to engineer about 300 features to use in our models to predict future lapses

We formed features by aggregating EMA items over various past time periods ranging from 12 - 168 hours in the past relative to the window we want to predict into.

We calculated mins, maxes and medians for the EMA items in these time periods

We also calculated counts of past lapses and counts of past EMAs completed to index engagment with our monitoring system.

And we included these scores both in raw form and as change from baselines for the participant based on all their previous responses since the start of the study.
:::

-----------------------------------------------------------------------------

## 1 Week: Probabilities for No Lapse and Lapse

::: {.columns}
:::: {.column width="50%"}
- Model predicts [probability]{style="color: orange;"} of lapse in next week for “[new]{style="color: orange;"}” observations in test sets

- Can panel predictions by [Ground Truth]{style="color: orange;"} (i.e., true lapse vs. no lapse observations

- Want high probabilities to be high for true lapses and low for true no lapses
::::

:::: {.column width="50%"}
<!--Lapse Probability by Ground Truth"-->
```{r}
#| label: fig_hist_week
#| fig-height: 6
#| fig-width: 8

source(here::here("figs/risk/figs_ema_probability.R"))
fig_hist_week
```
::::
:::

::: {.notes}
OK, lets begin to explore how well we can do.

lets start with the model that provides the coarsest level of temporal specificity – 1 week, and let me take a moment to make the predictions that this machine learning model provides more concrete for you

On the right, you are looking at histograms of the lapse probability predictions that the model makes for all the weeks for all the patients in the held out folds.   

I’ve paneled these histograms by whether a lapse did or did not happen in reality for each predicted week.  The top panel is for weeks with lapses and the bottom panel is for weeks with no lapses.

Ideally, you want the predicted probabilities to be very high for weeks when there was a lapse and very low for weeks when there was no lapse.    

And this is exactly what we see for the one week lapse window model
:::

-----------------------------------------------------------------------------

## Modeling: Area under ROC curve (auROC) 

::: {.columns}
:::: {.column width="50%"}
- 0.50 for random classifier
::::

:::: {.column width="50%"}
```{r}
#| label: figs_random_roc
#| fig-height: 6
#| fig-width: 6

tibble(model = c("random", "random"),
       specificity = c(1, 1),
       sensitivity = c(0, 0)) |> 
   mutate(model = factor(model, levels = c("random", "perfect"))) |> 
   plot_roc(line_colors = c("gray", "red"))
```
::::
:::

::: {.notes}
We will be evaluting our models with a performance metric called the Area under the Receiver Operating Characteristic curve or auROC for short.

The ROC curve plots a models true positive rate by its false positive rate.

The dotted line on the right shows the curve for a classifier with random features that are unrelated to the outcome.  For any increase in the true positive rate, there is a comparable increase in the false positive rate.

And the auROC for this random classifier would be .50
:::

-----------------------------------------------------------------------------

## Modeling: Area under ROC curve (auROC) 

::: {.columns}
:::: {.column width="50%"}
- [0.50 for random classifier]{style="color: gray;"}
- 1.00 for perfect classifer
::::

:::: {.column width="50%"}
```{r}
#| label: figs-perfect-roc-1
#| fig-height: 6
#| fig-width: 6

tibble(model = c("random", "random", "perfect", "perfect", "perfect"),
       specificity = c(1, 1, 1, 1, 0),
       sensitivity = c(0, 0, 0, 1, 1)) |> 
   mutate(model = factor(model, levels = c("random", "perfect"))) |> 
   plot_roc(line_colors = c("gray", "red"))
```
::::
:::

::: {.notes}
And the red line displays the curve for a perfectly performing classifer.  It pushes up into the top left corner of the plot where the true positve rate is 1.0 and the false positive rate is 0.   

This perfect classifer would have an auROC of 1.0
:::

-----------------------------------------------------------------------------

## Modeling: Area under ROC curve (auROC) 

::: {.columns}
:::: {.column width="50%"}
- [0.50 for random classifier]{style="color: gray;"}
- [1.00 for perfect classifer]{style="color: gray;"}
- Likelihood that model will assign a higher lapse probability to randomly selected lapse vs. no-lapse observation
::::

:::: {.column width="50%"}
```{r}
#| label: figs-perfect-roc-2
#| fig-height: 6
#| fig-width: 6

tibble(model = c("random", "random", "perfect", "perfect", "perfect"),
       specificity = c(1, 1, 1, 1, 0),
       sensitivity = c(0, 0, 0, 1, 1)) |> 
   mutate(model = factor(model, levels = c("random", "perfect"))) |> 
   plot_roc(line_colors = c("gray", "red"))
```
::::
:::

::: {.notes}
So the range of auROCs is approximately .5 - 1.0 

but the auROC also has an intutiive intepretation that I want you to understand.  

Its value represents the probability that any randomly selected positive observation, in our case, a lapse, will be assigned a higher probability score than a randomly selected negative observation where no lapse occurs.

So for example, if a model has an auROC of 0.90, it means that there is a 90 percent chance that any specific lapse will be scored higher than a specific no-lapse event 
:::

-----------------------------------------------------------------------------

## 1 Week: ROC Curve

::: {.columns}
:::: {.column width="50%"}
Model auROCs

- Next week = [0.90]{style="color: orange;"}; [0.88 - 0.91]

:::::{.callout-tip icon=false}
## Coarse rules of thumb for auROC 
.70 - .80 are considered fair  
.80 - .90 are considered good  
\> .90 are considered excellent  
:::::
::::

:::: {.column width="50%"}
<!--ROC Curve for Week-->
```{r}
#| label: fig_roc_week
#| fig-height: 6
#| fig-width: 6

# plot made in fig_probability.R
fig_roc_week
```
::::
:::

::: {.notes}
But we can get more precise in our evaluation of this model with the ROC curve and auROC.

Remember that the area under this curve can range from approximately .5 for a random classifier to 1.0 for a classifier that performs perfectly.  

And the AUC for our 1 week model is .90, which is generally considered excellent performance.  
:::

-----------------------------------------------------------------------------

## 1 Day: ROC Curve

::: {.columns}
:::: {.column width="50%"}
Model auROCs

- Next week = [0.90]{style="color: orange;"}; [0.88 - 0.91]

:::::{.callout-tip icon=false}
## Coarse rules of thumb for auROC 
.70 - .80 are considered fair  
.80 - .90 are considered good  
\> .90 are considered excellent  
:::::
::::

:::: {.column width="50%"}
<!--Blank-->
::::
:::

::: {.notes}
So we were very encouraged by the performance of this model because it exceeded the performance of the only other published week level lapse prediction model that we were aware of.   

But we were also eager to see how well we could do if we required a higher level of temporal precision by developing a day level model.
:::

-----------------------------------------------------------------------------

## 1 Day: ROC Curve

::: {.columns}
:::: {.column width="50%"}
Model auROCs

- Next week = [0.90]{style="color: orange;"}; [0.88 - 0.91]
- Next day  = [0.91]{style="color: green;"}; [0.89 - 0.92]

:::::{.callout-tip icon=false}
## Coarse rules of thumb for auROC 
.70 - .80 are considered fair  
.80 - .90 are considered good  
\> .90 are considered excellent  
:::::
::::

:::: {.column width="50%"}
<!--ROC Curve for Week & Day-->
```{r}
#| label: fig_roc_week_day
#| fig-height: 6
#| fig-width: 6

# plot made in fig_probability.R
fig_roc_week_day
```
::::
:::

::: {.notes}
And here is the ROC curve and the auROC for the day level model in green, superimposed on the week level model in orange

The day level model performed as well as, if not better, than the week level model with an auROC of .91

[PAUSE]
:::

-----------------------------------------------------------------------------

## 1 Hour: ROC Curve

::: {.columns}
:::: {.column width="50%"}
Model auROCs

- Next week = [0.90]{style="color: orange;"}; [0.88 - 0.91]
- Next day  = [0.91]{style="color: green;"}; [0.89 - 0.92]
- Next hour = [0.93]{style="color: blue;"}; [0.92 - 0.94]

:::::{.callout-tip icon=false}
## Coarse rules of thumb for auROC 
.70 - .80 are considered fair  
.80 - .90 are considered good  
\> .90 are considered excellent  
:::::
::::

:::: {.column width="50%"}
<!--ROC Curve for All-->
```{r}
#| label: fig_roc_all
#| fig-height: 6
#| fig-width: 6

# plot made in fig_probability.R
fig_roc_all
```
::::
:::

::: {.notes}
And it turns out that we can do somewhat better still with hour level predictions.  Here the blue curve represents the ROC curve for the hour level model, which had the best auROC yet, .93

[PAUSE]

And to be clear, we believe all three of these models to have clinically implementable levels of performance such that we can make meaningful decisions about the probability of a future lapse for individual participants at specfic moments or windows in time.

Thats the good news.
:::

-----------------------------------------------------------------------------

## Algorithmic Fairness

::: {.columns}
:::: {.column width="35%"}
::::

:::: {.column width="65%"}
::::
:::

::: {.notes}
[PAUSE]

But, not surprisingly given what I told you earlier about the characterisics of our training data, there is important bad news to share as well.

When we evaluate model performance, it is critical that we looks at performance in protected groups.  And too often, these analyses are not done or reported.  

Its only very recently that we have begin to take this seriously and we must.  If we hope to use our smart DTx to address existing dispartities in SUD then our models must perform well with all groups, regardless of their privilege or the use of these models may exacerbate rather than reduce existing mental healthcare disparities.
:::

-----------------------------------------------------------------------------

## Algorithmic Fairness

::: {.columns}
:::: {.column width="35%"}
- Substantially poorer performance if not white/non-hispanic

- Meaningfully poorer performance for other less priviledged groups

- Not just due to under-representation in training data
::::

:::: {.column width="65%"}
```{r}
#| label: figs_fairema_ci
#| fig-height: 6
#| fig-width: 8

source(here::here("figs/risk/figs_fairema_ci.R"))
cowplot::plot_grid(fig_race, fig_income, fig_sex, fig_age)
```

::::
:::

::: {.notes}
[PAUSE]

And our models have some serious but unfortunately not unexpected problems.

On the right, these are preliminary analyses looking at performance across a variety of binary groups defined by priviledge with respect to SUD treatment access and/or outcomes.  

From these analyses, it is clear that our models perform substantially worse when predicing lapse for anyone who isnt white and non-hispanic.  And this was expected given the lack of racial and ethinic diversity in the sample.

However, there are also some, though smaller, performance issues for other less privleged groups, even when we had reasonable divesity regarding those characteristics in the sample.  And this highlights other sources of potential bias. 

For example, when selecting which EMA questions to ask participants, we based this on domain expetise from decades of research on the risk factors for lapses.  However, that research was done with predominately white, predominately male participants who often had other priviledges that allowed for them to participate in research.  Given this, it may not be suprising that when we use this literature to select EMA items to measure, we may fail to include items that might tap lapse risk for people who are not white men.

I'll come back to this later when talking about our NIDA project and next steps and I hope we can discuss this too during the question period.
:::

-----------------------------------------------------------------------------

## Global Feature Importance

::: {.columns}
:::: {.column width="40%"}
- All EMA items impact lapse probability
::::

:::: {.column width="60%"}
```{r}
#| label: figs_global_all
#| fig-height: 6
#| fig-width: 8

source(here::here("figs/risk/figs_ema_shaps.R"))
fig_global_all
```

::::
:::

::: {.notes}
In the spirit of making this model more transparent and interpretable, lets also briefly look under the hood at global feature importance

The plot on the right shows feature names and their associated importance indexed by mean absolute SHAP values. The width of the bars shows the relative global importance of each feature for each model, globally across all participants and observations.  From this we see a few important characteristics of the model.   

First, all of EMA items affect predictions about lapse probability across observations  As you might expect, history of past lapses has a big influence on the probability of a future lapse.  But self reported abstinence efficacy, craving, history of stress events and other features from the EMA all make meaningful contributions to lapse probability across observations.  
:::

-----------------------------------------------------------------------------

## Global Feature Importance

::: {.columns}
:::: {.column width="40%"}
- [All EMA items impact lapse probability]{style="color: grey;"}
- Lapse day and Lapse hour are useful for day and hour level models as expected
::::

:::: {.column width="60%"}
```{r}
#| label: figs_global_all-2
#| fig-height: 6
#| fig-width: 8

fig_global_all
```
::::
:::

::: {.notes}
We also see that we can use lapse day and lapse hour to make predictions with the more temporally precise models.  Not surprisingly, people are more likely to lapse on weekends and during evening hours and the associated day and hour level models models can use that information to improve their lapse predictions.  

These features likely contribute to the superior performance of the hour and to a lesser extent the day model relative to the week interval prediction model.
:::

-----------------------------------------------------------------------------

## Global Feature Importance

::: {.columns}
:::: {.column width="40%"}
- [All EMA items impact lapse probability]{style="color: grey;"}
- [Lapse day and Lapse hour are useful for day and hour level models as expected]{style="color: grey;"}
- Demographics not particularly important (but limited race/ethnic diversity)
::::

:::: {.column width="60%"}
```{r}
#| label: figs_global_all-3
#| fig-height: 6
#| fig-width: 8

fig_global_all
```
::::
:::

::: {.notes}
And finally, we see that demographics were not particularly important for predicting lapses for these models. In other words, the frequency of observed lapses did not differ meaningfully by these demographic characteristics.  Though again, lets not forget the lack of racial and ethnic diversity in the sample.    
:::

-----------------------------------------------------------------------------

## (Selective) Next Steps

- Very strong overall performance
- Temporally precise models for immediate future lapse risk
- EMA risk features are intepretable and sensible

\

But...

- Critical to address issues of algorithmic fairness
- Need to develop broader set of (more passive) features
- Need to determine how to use these models for clinical benefit


::: {.notes}
So lets pause here for a quick re-cap of where we are so far

- We have models that predict exceptionally well when evaluted in the full sample

- These models have a high degree of temporal specificity, even down to hour level resolution

- The risk features from EMA map sensibly onto known lapse risks and we have interventions and supports designed to address many of these risks

- BUT, and this is a very big BUT, we have some critically important work to do with respect to algoritmic fairness before we can implement these models without doing harm.

Assuming we can do this in our subsequent projects, the next logical question is how can use these models.
:::

-----------------------------------------------------------------------------


## (Selective) Next Steps: Diversity and Fairness

1. Improve algorithmic fairness
   - Move beyond binary comparisons (e.g., issues of intersectionality) 
   - Explore computational solutions
   - More diversity in training data

::: {.notes}
:::

-----------------------------------------------------------------------------

## Active Project: Lapse in patients with Opioid Use Disorder 

::: {.medium}
- Recruiting ~ 300 patients in recovery from Opioid Use Disorder 
- National sample (size; diversity: demographics, location)
- More variation in stage of recovery (1 – 6 months at start)
- 12 months of monitoring
- Closer to real implementation methods
:::

::: {.columns}

:::: {.column width="60%"}
![risk2_pis.png](https://github.com/jjcurtin/slide_decks/blob/main/images/risk2_pis.png?raw=true)\ 
::::

:::: {.column width="40%"}
![nida_logo.png](https://github.com/jjcurtin/slide_decks/blob/main/images/nida_logo.png?raw=true)\ 
::::
:::


::: {.notes}
We are now collecting data for a NIDA funded project where we are specifically recruiting for racial, ethnic, and geographic diversity across the entire United States.


We are also recruiting for people at different stages in their recovery and following them for a longer period of time – up to 12 months
:::

-----------------------------------------------------------------------------

## (Selective) Next Steps: Sensing Geolocation and Communcations

1. [Improve algorithmic fairness]{style="color: gray;"}
   - [Move beyond binary comparisons (e.g., issues of intersectionality)]{style="color: gray;"}
   - [Explore computational solutions]{style="color: gray;"}
   - [More diversity in training data]{style="color: gray;"}
  
2.  Use geolocation and cellular communications
    - Increase performance?
    - Lower sensing burden
    - More (distinct) risk features for intervention recommendations!

::: {.notes}
First, as our sensing and recommendation system continues to mature, we will want a richer, broader set of lapse risk features so that we can distinguish better between different situations that require different supports. We can do this by engineering features from our location and communication signals, which tap into different experiences than what we measure by EMA.

And as an added benefit, the use of passive sensing rather than EMA may also lower the patient burden of using these systems long term.

Let's take a look at what we can get from geolocation and communications signals to provide you with some intuition about how we think this will work.
:::

-----------------------------------------------------------------------------

## {#gps_detection_1 data-menu-title="GPS detection, wide view" background-image="https://dionysus.psych.wisc.edu/present/john_gps_wide.png" background-size="100%" background-position="bottom" background-repeat="none"}


::: {.notes}
Here is a wide view of my moment-by-moment location detected by a GPS app over a month when we were first experimenting with this sensing method.  The app recorded the paths that I traveled, with movement by car in green and running in blue.

The red dots indicate places that I stopped to visit for at least a few minutes.

And although not displayed here, the app recorded the days and exact times that I was at each of these locations.

From these data, you can immediately see that I am runner, with long runs leaving from downtown Madison and frequent trail runs on the weekends in the county and state parks to the west and northwest.
:::

-----------------------------------------------------------------------------

## {#gps_detection_2 data-menu-title="GPS detection, zoomed" background-image="https://dionysus.psych.wisc.edu/present/john_gps_zoom.png" background-size="100%" background-position="bottom" background-repeat="none"}


::: {.notes}
Zooming in to the Madison isthmus, these data show that I drove my children halfway around the lake each morning to their elementary school.  And from these data we might be able to detect those stressful mornings when getting my young kids dressed and fed didn't go as planned and we were late, sometimes **very late**, to school!

The app recorded my daily running commute through downtown Madison to and from my office.  From this, we can observe my longs days at the office and also those days that I skipped out.

Looking at the red dots indicating the places I visit, the app can detect the restaurants, bars, and coffee shops where I eat, drink and socialize.  We can use public map data to identify these places and make inferences about what I do there.
:::

-----------------------------------------------------------------------------

## ...Imagine my text messages...

![smartphone_uber.png](https://github.com/jjcurtin/lectures/blob/main/images/smartphone_uber.png?raw=true){.absolute top="15%" left="33%" width="33%" height="auto"}\ 

::: {.notes}
In addition to geolocation, we also collected my smartphone communications logs and even the content of my text messages.

And no such luck, I don't plan to show you my actual text messages!

But imagine what we could learn about me from the patterns of my communications - Who I was calling, when I made those calls, and even the content of what I sent and received by text message.
:::

-----------------------------------------------------------------------------

## Context is Critical

<!--intentional blank page-->

::: {.notes}
We believe we can improve the predictive strength of these geolocation and communication signals even further by identifying the specific people and places that make us happy or sad or stressed, those that we perceive support our mental health and recovery and those who undermine it.
:::

-----------------------------------------------------------------------------

## Context is Critical
![smartphone_context.png](https://github.com/jjcurtin/lectures/blob/main/images/smartphone_context.png?raw=true){.absolute top="15%" left="33%" width="33%" height="auto"}\ 

::: {.notes}
For example, consider the implications of this brief text message thread between a hypothetical patient and their drinking buddy for what you might predict for the probability that they might lapse back to drinking in the coming hours.   

[PAUSE]

... And how would your prediction change if this wasn’t their drinking buddy but instead, their mom who was a big supporter of their recovery.

This interpersonal context matters!!!
:::

-----------------------------------------------------------------------------

## Context is Critical

<!--intentional blank page-->


::: {.notes}
We gather this contextual information quickly by asking a few key questions about the people and places we interact with frequently over the first couple of months that we record these signals.  And we can identify these frequent contacts and locations directly from these signals.

In our current projects, we target people and places that we interact with at least twice a month or more for more detailed follow-up to gather context.    And it turns out that this really isn’t that burdensome.   Most of us are creatures of habit and if we set a threshold for 2x monthly interactions, we typically only have 10-30 people and places that meet this threshold.   And it’s the same people and places each month so we can build this context up when the person first starts to use the system and after that it only needs to be updated occasionally when we go somewhere new or make a new friend.
:::

-----------------------------------------------------------------------------

## Contextualized Geolocation

- Location type (e.g., home, home of friend, bar, restaurant, liquor store, work, health care, AA/recovery meeting, gym/fitness center)
- Is [alcohol available]{style="color: blue;"} at this location
- Have you [drank alcohol]{stype="color: blue;"} at this location?
- Is your [experience at this location]{style="color: blue;"} generally pleasant, unpleasant, mixed or neutral?
- This location is ([high risk, moderate risk, low risk, no risk]{style="color: blue;"}) for my recovery

::: {.notes}
:::

-----------------------------------------------------------------------------

## Contextualized Communications

- Have you [drank alcohol]{style="color: blue;"} with this person?
- What is their [drinking status]{style="color: blue;"} (e.g., drinker, non-drinker)?
- Would you expect them to [drink in your presence]?{style="color: blue;"}
- Are they currently [in recovery]{style="color: blue;"} from alcohol or other substances?
- Do they [know about your recovery goals]{style="color: blue;"} and if so, [are they supportive]{style="color: blue;"}?
- Are your [experiences with them]{style="color: blue;"} typically pleasant, unpleasant, mixed or neutral?

::: {.notes}
TALK FIRST ABOUT CONTEXT

THEN TALK ABOUT PRELIMINARY ANALYSES
:::

-----------------------------------------------------------------------------

## (Selective) Next Steps: How to Yield Clinical Benefits 

1.  [Improve algorithmic fairness]{style="color: gray;"}
    - [Move beyond binary comparisons (e.g., issues of intersectionality)]{style="color: gray;"}
    - [Explore computational solutions]{style="color: gray;"}
    - [More diversity in training data]{style="color: gray;"}
  
2.  [Use geolocation and cellular communications]{style="color: gray;"}
    - [More (distinct) risk features for intervention recommendations]{style="color: gray;"}
    - [Lower sensing burden]{style="color: gray;"}

3.  Implement Recovery Monitoring and Support System for Patients
    - Who to provide model output to?
    - What output to provide? 
    - How to increase engagement, trust, utility, and benefits?
  
-----------------------------------------------------------------------------

## Clinical Uses: DO NOT provide model output to clinicians 

DO NOT provide model output to clinicians

- Clinicians are over-burdened
- Not ready for new data streams

::: {.notes}
When we started this work, we believed we were building these models to inform clinicians about their caseload.  

Today's digital therapeutics have clinician dashboards built into them and we, perhaps naively, thought clinicians could use this information to priortize their resources to patients who had the greatest need.

But as we talked to clinicians, it became very clear that they do not want any more info at this point.  

Post-pandemic, they are barely keeping their heads above water and are definitely not ready to add new systems and data streams in place.

However, in constrast, our work with participants suggested that they did see potential value in monitoring their recovery using our sensing system and prediction system.   So we have pivoted to considering what might be useful to provide directly to them.
:::

-----------------------------------------------------------------------------

## Clinical Uses: DO NOT Predict Lapses

::: {.columns}
:::: {.column width="50%"}
DO NOT predict class labels  (Lapse vs. No-Lapse)

- Iatriogenic effects?
- Information loss
::::

:::: {.column width="50%"}
<!--Blank-->
::::
:::

::: {.notes}
But lets again first start with how we should NOT use these models with individuals with SUDs.

We have been focusing on the lapse probabilities that are natively output from our prediction models.   However, its common to use these models to predict formal class labels.  In other words, to specifically predict that a lapse will happen or not.  Basically, a threshold is set, for example, 0.5, and if the probability of a lapse exceeds that threshold the model predicts that a lapse will occur.  Otherwise, it predicts that no lapse will occur

But there are several reasons that we DO NOT want to predict dichotomous class labels.

To start, there may be concerns about possible iatriogenic effects associated with telling a person that they are going to lapse. Or at least this should be a risk that is carefully considered if we provide these blunt dichotomous predictions to individuals.

Second, these days, I think we have all come to understand that taking scores that are natively quantitative like probabilities are, and artificially dichotomizing them results in substantial loss of information that is potentially valuable.
:::

-----------------------------------------------------------------------------

## Clinical Uses: DO NOT Predict Lapses

::: {.columns}
:::: {.column width="45%"}
DO NOT predict class labels  (Lapse vs. No-Lapse)

- [Iatriogenic effects]{style="color: gray;"}
- [Information loss?]{style="color: gray;"}
- Low positive predictive values 
  - Lapses on only ~7% of days
  - Spec and Sens > 0.80
  - PPV ~ 0.27
::::

:::: {.column width="55%"}
```{r}
#| label: fig-cm_day
#| fig-height: 6
#| fig-width: 6

fig_cm_day
```

::::
:::

::: {.notes}
But perhaps most importantly, when we are predicting class labels and the labels are highly unbalanced, the positive predictive value of those labels will low.

To make this concrete, on the right I am showing you a confusion matrix for class label predictions from the day model.   The columns represent reality, whether the observation was a true lapse on the right or a true no lapse on the left.  And the rows represent class predictions from the model made using a sensible threshold.

As you might expect, these data are unbalanced.  Only 7 percent of observations for any day are true lapses.  That is shown by the right column being narrower than the left.

This model has good specificity.  If you consider the left column, you see that more than 80% of no-lapse days are predicted to be no-lapses.   And it has good sensitivity.  If you consider the right column, you see that the model correctly identifies > 80% of the true lapses as well.

But if we look instead at the rows of matrix, and focus on the bottom row that represents when the model predicts a lapse, we see the problem with binary labels.   Even though only a small portion of the true no-lapses are false positives, shown in the bottom left cell, this number overwhelms the true positives in the bottom right cell because there just arent that many positive cases to detect.

This means that when the model predicts a lapse, it will be correct less than a third of the time.  

Obviously this makes the use of these class labels problematic to say the least.

To be clear, there are methods we can use to increase PPV but given the other reasons to avoid predicting labels we are not pursing them.
:::

## Clinical Uses: DO Use Lapse Probabilities

::: {.columns}
:::: {.column width="50%"}
DO use lapse probability

- auROCs range from 0.90 - 0.93
::::

:::: {.column width="50%"}
<!-- blank  -->
::::
:::

::: {.notes}
Instead of using the models to predict class labels, we believe that there is potentially high value in using the original lapse probabilities directly output by the model.

I've already shown you that these probabilties can discriminate very well between lapse and no-lapse observations, correctly assigning a higher probability to lapses more than 90% of the time.
:::

-----------------------------------------------------------------------------

## Clinical Uses: DO Use Lapse Probabilities

::: {.columns}
:::: {.column width="50%"}
DO use lapse probability

- [auROCs range from 0.90 - 0.93]{style="color: gray;"}
- Probabilities are calibrated and ordinal
- Provides fine gradations of relative risk for clinical decision-making
::::

:::: {.column width="50%"}
```{r}
#| label: fig_cal
#| fig-height: 6
#| fig-width: 6

fig_cal
```
::::
:::

::: {.notes}
And critically, these probabilities are very well calibrated and at least ordinal in their relationship with the true probability that a lapse will occur.   

On the right, I am showing you a simple calibration plot.  On the x-axis, I've binned predicted lapse probabilities into bin widths of 10 percent and for each of these bins, I display the actual observed probability of lapses for observations in that bin.  

If the probabilities were perfectly calibrated, the bin means would all fall on the dotted line with the bin from 0 - 10 having an observed probilitiy of .05, the bin from 10 - 20 having a probability of .15, and so on.

And this is essentially what we see for our models.

Given this, we believe that the lapse probabilities can provide precise, fine gradations of risk for clinical decision making.
:::

-----------------------------------------------------------------------------

## Clinical Uses: Who and When?

::: {.columns}
:::: {.column width="50%"}
- Day and Hour models can be used for "just-in-time" interventions
::::

:::: {.column width="50%"}
<!-- blank  -->
::::
:::

::: {.notes}
So let's consider first how to use the temporal information provided by the three models.

FREE TALK
:::

-----------------------------------------------------------------------------

## Clinical Uses: Who and When?

::: {.columns}
:::: {.column width="50%"}
- [Day and Hour prediction windows can be used for "just-in-time" interventions]{style="color: gray;"}
- Week window may be too coarse for JIT
- Push week window further into the future for support outside of DTx 
::::

:::: {.column width="50%"}
```{r}
#| label: figs_lag_ci
#| fig-height: 6
#| fig-width: 6

source(here::here("figs/risk/figs_lag_ci.R"))
fig_lag_ci
```
::::
:::

::: {.notes}
FREE TALK
:::

-----------------------------------------------------------------------------

## Clinical Uses: Which Interventions/Supports?

::: {.columns}
:::: {.column width="50%"}
- Tools from interpretable AI can help explain [why]{style="color: orange;"} a future lapse is expected
::::

:::: {.column width="50%"}
<!--blank-->
::::
:::

::: {.notes}
We can get more than just when lapses are probable from these models.

We can also begin to use tools from the emerging field of interpretable AI to understand WHY they may occur
:::

-----------------------------------------------------------------------------

## Clinical Uses: Which Interventions/Supports?

::: {.columns}
:::: {.column width="50%"}
- [Tools from interpretable AI can help explain why a future lapse is expected]{style="color: gray;"}
- Local SHAP values highlight important risk features for a [specific individual at a specific moment in time]{style="color: orange;"}
::::

:::: {.column width="50%"}
```{r}
#| label: figs_global_day
#| fig-height: 6
#| fig-width: 6

fig_global_day
```
::::
:::

::: {.notes}
FREE TALK
:::

-----------------------------------------------------------------------------

## Clinical Uses: Which Interventions/Supports?

::: {.columns}
:::: {.column width="50%"}
- [Tools from interpretable AI can help explain why a future lapse is expected]{style="color: gray;"}
- [Local SHAP values highlight important risk features for a specific individual at a specific moment in time]{style="color: gray;"}
- If we know [why]{style="color: orange;"}, we can begin to [recommend interventions and supports]{style="color: orange;"} to address these risks
::::

:::: {.column width="50%"}
```{r}
#| label: figs_global_day-2
#| fig-height: 6
#| fig-width: 6

fig_global_day
```
::::
:::

::: {.notes}
- For example, today one DTx user may show a high lapse probability and the model may have assigned that probability because they have been craving a lot recently. For that person, we could recommend urge surfing techniques and provide them support doing it within the DTx. 

- A second person might have similarly high lapse probability but instead because they have lapsed a few times in recent weeks. They could be encouraged to complete activities designed to increase their motivation for abstinence. 

- A third person might have a low probability of lapsing because they have reported many recent positive activities. No intervention might be needed for this person but they could receive feedback about how their commitment to their well-being was paying off for their recovery.


SLIDE CONTINUE ON NEXT CLICK
:::

-----------------------------------------------------------------------------

## Clinical Uses: Which Interventions/Supports?

::: {.columns}
:::: {.column width="50%"}
- [Tools from interpretable AI can help explain why a future lapse is expected]{style="color: gray;"}
- [Local SHAP values highlight important risk features for a specific individual at a specific moment in time]{style="color: gray;"}
- If we know [why]{style="color: orange;"}, we can begin to [recommend interventions and supports]{style="color: orange;"} to address these risks
::::

:::: {.column width="50%"}
```{r}
#| label: figs_global_day-3
#| fig-height: 6
#| fig-width: 6

fig_global_day
```
::::
:::

::: {.notes}
As a starting point, the mapping between between important local risk features and specific interventions or supports could be created using clinical domain expertise. In other words, what would a clinician tell their patient to do in those circumstances. We can simply hard code these clinically derived mappings between risk features and support recommendations in the Smart DTx

However, with enough training data, reinforcement learning can also be applied to this problem to allow the model to learn the best intervention to recommend given a set of risk features to reduce subsequent lapse probability.

We are really excited about these possiblities so let me tell you more about where we are going from here
:::

-----------------------------------------------------------------------------


-----------------------------------------------------------------------------

## (Selective) Next Steps:  Algorithmic Feedback

- [Use geolocation and cellular communications]{style="color: gray;"}
- [Improve algorithmic fairness]{style="color: gray;"}
- Optimize algorithmic feedback to increase trust


::: {.notes}
INTROCE IDEAL THAT PEOPLE NEED TO TRUST THE FEEDBACK FROM OUR MODELS
:::


-----------------------------------------------------------------------------

## Optimize Algorithmic Feedback in Smart DTx

- Daily updates from Day model based on EMA and and geolocation features
  - Lapse probability for that day
  - Recent trends in lapse probabilitity 
  - Locally important features for that day

- Manipulate algorithmic transparency in daily message

- Measure trust, risk-relevant engagement, and clinical outcomes

![niaaa_logo.png](https://github.com/jjcurtin/lectures/blob/main/images/niaaa_logo.png?raw=true){.absolute bottom="5%" right="2%" width="auto" height="auto"}\ 

::: {.notes}

:::

-----------------------------------------------------------------------------

## CRediTs

![credits.png](https://github.com/jjcurtin/lectures/blob/main/images/credits_risk1.png?raw=true){.absolute bottom="15%" left="5%" width="92%" height="auto"}\ 
