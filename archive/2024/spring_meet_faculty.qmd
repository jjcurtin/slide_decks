---
title: "Precision Mental Health for SUD: Contributions from High Dimensional Prediction Models" 
author: "John J. Curtin, Ph.D."
institute: "University of Wisconsin-Madison"
title-slide-attributes:
  data-background-image: https://github.com/jjcurtin/lectures_science/blob/main/images/smartphone_know_you.png?raw=true
  data-background-size: 35%
  data-background-repeat: no
  data-background-position: left 10% bottom 10%
format: 
  revealjs:
    scrollable: true
    chalkboard: true
    css: slides.css
fig-cap-location: top
editor_options: 
  chunk_output_type: console
---

## Explanations vs. Prediction in Applied? Psychology


Experimental Psychopathology promised:

- Experiments will allow us to develop theories about mechanisms that cause, maintain, or correct psychopathology (proxy manipulations and measures)
- Findings from experiments will generalize to real world psychopathology 
- Solid theory about these mechanisms would lead to treatments
- Biologically based approaches would allow us to "carve nature at its joints"


## And....?

- Tom Insel, former Director of NIMH (Wired Magazine, 2017)

> “I spent 13 years at NIMH really pushing on the neuroscience and genetics of mental disorders, and when I look back on that I realize that while I think I succeeded at getting lots of really cool papers published by cool scientists at fairly large costs —I think $20 billion— I don’t think we moved the needle in reducing suicide, reducing hospitalizations, improving recovery for the tens of millions of people who have mental illness.” 


## And What About Me....?

John Curtin, 1998 - 2019 

- Curtin J, Lang A, Patrick C, Stritzke W (1998). Alcohol and fear-potentiated startle: The role of competing cognitive demands in the stress-reducing effects of intoxication. **Journal of Abnormal Psychology**, 107, 547-557. 

- Curtin JJ, Patrick CJ, Lang AR, Cacioppo JT, Birbaumer N. (2001). Alcohol affects emotion through cognition. **Psychological Science**, 12, 527-531.

- Piper ME, Curtin JJ (2006). Tobacco withdrawal and negative affect: An analysis of initial emotional response intensity and voluntary emotion regulation. **Journal of Abnormal Psychology**, 115, 96-102. 

- Curtin JJ, Lang AR (2007). Alcohol and emotion: Insights and directives from affective science. In J. Rottenberg & Johnson, S (Eds.), **Emotion and Psychopathology: Bridging Affective and Clinical Science**, (pp. 191-213), American Psychological Association.

- Moberg CA, Curtin JJ (2009). Alcohol selectively reduces anxiety but not fear: Startle response during unpredictable vs. predictable threat. **Journal of Abnormal Psychology**, 118, 335-347.

- Hogle JM, Kaye JT, Curtin JJ (2010). Nicotine withdrawal increases threat-induced anxiety but not fear: Neuroadaptation in human addiction. **Biological Psychiatry**, 68, 719-725.

- Moberg CA, Weber S, Curtin JJ (2011). Alcohol dose effects on stress response to cued threat vary by threat intensity. **Psychopharmacology**, 218, 217-227. 

- Hefner KR, Curtin JJ (2012). Alcohol stress response dampening: Selective reduction of anxiety in the face of uncertain threat. **Journal of Psychopharmacology**, 26, 232 – 245.

- Hefner KR, Moberg CA, Hachiya LY, Curtin JJ (2013). Alcohol stress response dampening during imminent vs. distal, uncertain threat. **Journal of Abnormal Psychology**, 122, 756-769.

- Bradford DE, Shapiro B, Curtin JJ (2013). How bad could it be? Alcohol dampens stress responses to uncertain intensity threat. **Psychological Science**, 24, 2541-2549. 

- Bradford DE, Kaye JT, Curtin JJ (2014). Not just noise: Individual differences in general startle reactivity predict startle reactivity to uncertain and certain threat. **Psychophysiology**, 51, 407-411. 

- Bradford DE, Starr MJ, Shackman AJ, Curtin JJ (2015). Empirically based comparisons of the reliability and validity of common quantification approaches for eyeblink startle potentiation in humans. **Psychophysiolology**, 52, 1669-1681. 

- Bradford DE, Curtin JJ, Piper ME (2015). Anticipation of smoking sufficiently dampens stress reactivity in nicotine deprived smokers. **Journal of Abnormal Psychology**, 124, 128-36. 

- Kaye JT, Bradford DE, & Curtin JJ (2016). Psychometric properties of startle and corrugator response in NPU, Affective Picture Viewing, and Resting State tasks. **Psychophysiology**, 53, 1241-1255. 

- Bradford DE, Motschman CA Starr MJ, & Curtin JJ (2017). Alcohol’s effects on emotionally motivated attention, defensive reactivity, and subjective anxiety during uncertain threat. **Social Cognitive and Affective Neuroscience**, 12, 1823–1832. 

- Kaye JT, Bradford DE, Magruder KP & Curtin JJ (2017). Probing for neuroadaptations to unpredictable stressors in addiction: translational methods and emerging evidence. **Journal of Studies on Alcohol and Drugs**, 78, 353-371.

- Moberg CA, Bradford DE, Kaye JT, & Curtin JJ (2017). Increased startle potentiation to unpredictable stressors in alcohol dependence: Possible stress neuroadaptation in humans. **Journal of Abnormal Psychology**.

- Hur J, Kaplan CM, Smith JF, Bradford DE, Fox AS, Curtin JJ, Shackman AJ (2018). Acute alcohol administration dampens central extended amygdala reactivity. **Scientific Reports**, 12, 16702. +

- Hefner KR, Starr MJ, & Curtin JJ (2018). Heavy marijuana use but not deprivation is associated with increased stressor reactivity. **Journal of Abnormal Psychology**, 127, 348-358. 

- Kaye JT, Fronk GE, Zgierska AE, Cruz MR, Rabago D, Curtin JJ (2019). Acute prazosin administration does not reduce stressor reactivity in healthy adults. **Psychopharmacology**, 236, 3371-3382. 

## Pitfalls of Explanatory Research in Experimental Psychopathology {.smaller}

- Laboratory manipulations and measures do not predict substantial variance in real world behaviors/outcomes

- Our theories and mechanisms built on these laboratory approaches do no explain substantial variance in real world behaviors/outcomes

- Focus on one to several mechanisms would not be expected to explain the emergence, maintenance or remediation of complex thoughts, feelings, and behaviors associated with psychopathology

- These approaches are unlikely to produce interventions that yield meaningful clinical benefits


## Similar Conclusions from Others

Eisenberg, IW, Bisset, PG, Enkavi, AZ, Li, J, MacKinnon, DP, Marsch, LA, Poldrack, RA (2019).  Uncovering the structure of self-regulation through data-driven ontology discovery. Nature Communications, 10,  2319
\
\
**Psychological sciences have identified a wealth of cognitive processes and behavioral phenomena, yet struggle to produce cumulative knowledge**. 

**Progress is hamstrung** by siloed scientific traditions and **a focus on explanation over prediction**, two issues that are **particularly damaging for the study of multifaceted constructs like self-regulation**. 

Here, we derive a psychological ontology from a study of individual differences across a broad range of behavioral tasks, self-report surveys, and self-reported real-world outcomes associated with self-regulation. **Though both tasks and surveys putatively measure self-regulation, they show little empirical relationship**. Within tasks and surveys, however, the ontology identifies reliable individual traits and reveals opportunities for theoretic synthesis. We then evaluate predictive power of the psychological measurements and find that **while surveys modestly and heterogeneously predict real-world outcomes, tasks largely do not**. We conclude that self-regulation lacks coherence as a construct, and that data-driven ontologies lay the groundwork for a cumulative psychological science.


## Similar Conclusions from Others

Yarkoni T., Westfall, J (2017). Choosing Prediction Over Explanation in Psychology: Lessons From Machine Learning.  Perspectives on Psychological Science.  12, 1100-1122.
\
\
Psychology has historically been concerned, first and foremost, with **explaining the causal mechanisms that give rise to behavior**. 

**Randomized, tightly controlled experiments are enshrined as the gold standard of psychological research**, and there are endless investigations of the various mediating and moderating variables that govern various behaviors. 

We argue that psychology's near-total focus on explaining the causes of behavior has led much of the field to be populated by research programs that **provide intricate theories of psychological mechanism but that have little (or unknown) ability to predict future behaviors with any appreciable accuracy**. 

We propose that principles and techniques from **the field of machine learning can help psychology become a more predictive science**. We review some of the fundamental concepts and tools of machine learning and point out examples where these concepts have been used to conduct interesting and important psychological research that focuses on predictive research questions. We suggest that an **increased focus on prediction, rather than explanation, can ultimately lead us to greater understanding of behavior.**

## What Might Applied Predictive Science Offer?

Not sure about *greater understanding of behavior*
\
\
But...

- if we can predict treatment efficacy, we can improve treatment outcomes by selecting optimal treatments to patients

- if we can predict future symptoms or harmful behaviors, we can intervene "just-in-time" to prevent their occurence

- better still if we can combine **just-in-time** with **optimal treatments**

## Precision Medicine

Barak Obama, Remarks by the President on Precision Medicine, January 2015

>  "And that’s the promise of precision medicine... <br>
-- delivering the right treatments, <br>
-- at the right time, <br>
-- every time <br>
-- to the right person"


**Precision Mental Health** may present the biggest opportunities for impact

- Many treatments and supports available
- Treatments are modestly effective
- Symptoms and harmful behaviors are chronic and time-varying


## Project Match

We have three modestly effective (multi-faceted) treatments for AUD

- Motivational Enhancement Therapy
- Cognitive Behavioral Therapy
- 12-Step Facilitation

Goal: Match patients to optimal treatments based on individual differences based on ID X Treatment interactions
\
\
Considered hundreds of IDs

- Demographics
- Personality traits
- AUD history


## Project Match

RB Cutler, 2005 (BMC Public Health)

> "Project MATCH was the largest and most expensive alcoholism treatment trial ever conducted. The results were disappointing. There were essentially no patient-treatment matches, and three very different treatments produced nearly identical outcomes. <br><br>
An analysis of the problem suggested that too many Type I errors were being made in the alcoholism literature. Type I errors typically occur when an inappropriately large number of statistical tests are performed"

- Predicting treatment effectiveness with single characteristics is insufficient
- Analytic approaches of the 90s limited matching to simple Characteristic X Tx interactions and testing these interactions separately (typical explanatory approach)
- We can do better now with high dimensional characterization and machine learning techniques


## High Dimensional Tx Matching 

- Conduct detailed, comprehensive baseline assessments of individual differences and recent experiences 
- Use these characteristics and recent experiences to develop high dimensional models to predict meaningful treatment outcomes (real world)
- Include first line (and other?) treatments received in these prediction models
- Allow for interactions among baseline characteristics/experiences and treatments 
- Models can then be used to predict expected outcome for patient for all available treatments
- Select the optimal treatment for them

## High Dimensional Tx Matching in the Hands of Gaylen Fronk

- ~ 1100 smokers pooled across three first line medications for smoking cessation
- Many, many individual differences collected at baseline (prior to treatment assignment)
- Point prevalence abstinence measured at 
  - 4 weeks
  - 12 weeks (end of treatment)
  - 24 weeks

- Developed model to predict 4 week outcomes
  - Elastic Net GLM (essentially LASSO)
  - Hundreds of features
  - Interactions with medication
  
- Use model to predict the probability of abstinence for each smoker for each medication
- Use probability to rank their optimal, intermediate, and worst medication
- Compare outcomes for smokers who received optimal vs. intermediate vs. worst medication (and RCT outcomes overall)

  
## Outcomes for Model-Based "Precise" Tx Recommendations

:::: {.rows}

::: {.row height="20%"}
- Smoking abstinence ~ 33% for RCT assigned TX 
- Likely similar if clinician selected because these are comparable front line TX?
:::

::: {.row height="80%"}
{{< embed ../../../notebooks/match_figs.qmd#fig-tx_week_1 >}}
:::

::::

## Outcomes for Model-Based "Precise" Tx Recommendations

:::: {.rows}

::: {.row height="20%"}
- More than 10% increase in abstinence if received model-recommended "optimal" TX
- Clinically meaningful increase given baseline efficacy of ~ 33%
:::

::: {.row height="80%"}
{{< embed ../../../notebooks/match_figs.qmd#fig-tx_week_2 >}}
:::

::::

## Outcomes for Model-Based "Precise" Tx Recommendations

- Relative increase for model-recommended optimal TX sustained for six months
- Magnitude of benefit decreases

{{< embed ../../../notebooks/match_figs.qmd#fig-tx_week_3 >}}

## Outcomes for Model-Based "Precise" Tx Recommendations

- Model can distinguish between optimal, intermediate, and worst TX
- Can allow some informed choice by patient

{{< embed ../../../notebooks/match_figs.qmd#fig-tx_week_4 >}}

## Outcomes for Model-Based "Precise" Tx Recommendations

- Distinctions between ranked TX also diminish with time
- Also expected because baseline characteristics are not static but not updated

{{< embed ../../../notebooks/match_figs.qmd#fig-tx_week_5 >}}



## Precision Mental Health

Super excited but....
\
\
Barak Obama, Remarks by the President on Precision Medicine, January 2015

>  "And that’s the promise of precision medicine... <br>
-- delivering the right treatments, <br>
-- at the right time, <br>
-- every time <br>
-- to the right person"

\
\

- Baseline characteristics don't predict well forever
- Life happens
- Events and experiences matter
- Prediction models need updated inputs to continue to perform well

## Enter Personal Sensing

What is personal sensing

- Measurement embedded in day-to-day life
- Momentary (or very recent) experiences and events
- Longitudinal
- Minimally invasive


Some options

- Ecological Momentary Assessments
- Sensors (physiology, sleep)
- Contextualized Geolocation
- Contextualized Communications (smartphone; social media)


## {#gps_detection_1 data-menu-title="GPS detection, wide view" background-image="https://dionysus.psych.wisc.edu/present/meet_faculty/images/john_gps_wide.png" background-size="100%" background-position="bottom" background-repeat="none"}


::: {.notes}
Here is a wide view of my moment-by-moment location detected by a GPS app over a month when we were first experimenting with this sensing method.  The app recorded the paths that I traveled, with movement by car in green and running in blue.

The red dots indicate places that I stopped to visit for at least a few minutes.

And although not displayed here, the app recorded the days and exact times that I was at each of these locations.

From these data, you can immediately see that I am runner, with long runs leaving from downtown Madison and frequent trail runs on the weekends in the county and state parks to the west and northwest.
:::

## {#gps_detection_2 data-menu-title="GPS detection, zoomed" background-image="https://dionysus.psych.wisc.edu/present/meet_faculty/images/john_gps_zoom.png" background-size="100%" background-position="bottom" background-repeat="none"}


::: {.notes}
Zooming in to the Madison isthmus, these data show that I drove my children halfway around the lake each morning to their elementary school.  And from these data we might be able to detect those stressful mornings when getting my young kids dressed and fed didn't go as planned and we were late, sometimes **very late**, to school!

The app recorded my daily running commute through downtown Madison to and from my office.  From this, we can observe my longs days at the office and also those days that I skipped out.

Looking at the red dots indicating the places I visit, the app can detect the restaurants, bars, and coffee shops where I eat, drink and socialize.  We can use public map data to identify these places and make inferences about what I do there.
:::

## ...Imagine my smartphone communications...{.smaller}

![smartphone_uber.png](https://github.com/jjcurtin/lectures/blob/main/images/smartphone_uber.png?raw=true){.absolute top="15%" left="33%" width="33%" height="auto"}\ 

::: {.notes}
In addition to geolocation, we also collected my smartphone communications logs and even the content of my text messages.

And no such luck, I don't plan to show you my actual text messages!

But imagine what we could learn about me from the patterns of my communications - Who I was calling, when I made those calls, and even the content of what I sent and received by text message.

:::

## Context is Critical


::: {.notes}
We believe we can improve the predictive strength of these geolocation and communication signals even further by identifying the specific people and places that make us happy or sad or stressed, those that we perceive support our mental health and recovery and those who undermine it.

:::

## Context is Critical
![smartphone_context.png](https://github.com/jjcurtin/lectures/blob/main/images/smartphone_context.png?raw=true){.absolute top="15%" left="33%" width="33%" height="auto"}\ 

::: {.notes}


For example, consider the implications of this brief text message thread between a hypothetical patient and their drinking buddy for what you might predict for the probability that they might lapse back to drinking in the coming hours.   

... And how would your prediction change if this wasn’t their drinking buddy but instead, their mom who was a big supporter of their recovery.

This interpersonal context matters!!!

:::

## Context is Critical

<!--intentional blank page-->


::: {.notes}
We can gather this contextual information quickly by asking a few key questions about the people and places we interact with frequently over the first couple of months that we record these signals.  And we can identify these frequent contacts and locations directly from these signals.

In our current projects, we target people and places that we interact with at least twice a month or more for more detailed follow-up to gather context.    And it turns out that this really isn’t that burdensome.   Most of us are creatures of habit and if we set a threshold for 2x monthly interactions, we typically only have 10-30 people and places that meet this threshold.   And it’s the same people and places each month so we can build this context up when the person first starts to use the system and after that it only needs to be updated occasionally when we go somewhere new or make a new friend.

:::

## Contextualized Geolocation

![context_gps.png](https://github.com/jjcurtin/lectures/blob/main/images/context_gps.png?raw=true){width="75%"}\ 

::: {.notes}

:::

## Contextualized Communications

![context_cell.png](https://github.com/jjcurtin/lectures/blob/main/images/context_cell.png?raw=true){width="75%"}\ 

::: {.notes}

:::


## Lapse Prediction in Patients with AUD

> “Could you predict not only [who]{.red} might be at greatest risk for relapse … <br>
 … but precisely [when]{.red} that relapse might occur … <br>
 … and how [best to intervene]{.red} to prevent it?"


## Lapse Prediction in Patients with AUD

:::: {.columns}

::: {.column width="60%"}
- 151 patients with AUD
- Early in recovery (1-8 weeks)
- Committed to abstinence throughout study
- Followed for up to 3 months
- Collected active and passive personal sensing data streams
:::

::: {.column width="40%"}
![risk1_pis.png](https://github.com/jjcurtin/lectures/blob/main/images/risk1_pis.png?raw=true)\    
![niaaa_logo.png](https://github.com/jjcurtin/lectures/blob/main/images/niaaa_logo.png?raw=true)\   
:::


::::
[GOAL:]{.red} Develop a temporally precise lapse monitoring (prediction) system for patients with AUD




::: {.notes}
So let me transition now to describing how we are taking the first baby steps toward developing smart digital therapeutics for SUD

We have recently completed a NIAAA funded project where we collected data from 151 participants who were in early recovery from a moderate to severe alcohol use disorder.  

These participants were committed to abstinence at the start of the study and we followed them for up to 3 months, collecting a variety of active and passive personal sensing data streams.

Our first goal with this grant was to develop machine learning algorithms that can generate temporally precise predictions about when future lapses back to alcohol use will occur for patients with AUD.

:::

## Personal Sensing Data Streams

- [4X daily ecological momentary assessments (EMA)]{.red}

- Monthly self-report

- Geolocation (GPS)

- Cellular communications (voice and text messages)
  - Meta data
  - Text message content

- Sleep sensor (Wake/sleep times; sleep efficiency; wakings; restlessness)

::: {.notes}
As I mentioned, in this project, we collected a variety of active and passive personal sensing data streams.   

Participants completed brief (7-10 item) ecological momentary assessments or EMAs, 4 times per day

We also have 

- more temporally coarse, monthly self reports, 

- Moment by moment geolocation,

- Meta data from their cellular communications and the actual content of their text messages, 

- and we had sleep sensors in their beds.

We are in the early stages of model building at this point and I will focus today on results from preliminary models using only EMA.  However, we are actively working with GPS and cellular communications as well and I’ll end with some brief discussion of those preliminary models, as well as early plans on how to implement these models clinically to help patients.
:::

## 4x Daily Ecological Momentary Assessments {.medium}


:::: {.columns}

::: {.column width="60%"}
![risk1_ema_questions.png](https://github.com/jjcurtin/lectures/blob/main/images/risk1_ema_questions.png?raw=true){.absolute width="60%" top="15%" height="auto"}\    

:::
::: {.column width="30%"}

- Current
  - Craving
  - Affect
  - Risky situations
  - Stressful events
  - Pleasant events

- Future
  - Risky situations
  - Stressful events
  - Confidence
:::
::::

::: {.notes}
So let me tell you a bit more about the 4x daily EMA we collected.  

On each EMA, participants reported the date and time of any lapses back to alcohol use that they hadn’t previously reported. 

All of the EMAs also asked them about their current craving, affective valence and arousal, recent risky situations, and recent stressful and pleasant events since their last EMA.

On the first EMA each day, they also reported any future risky situations and stressful events that they expected in the next week and their confidence that they would remain abstinent. 
:::

## Feature Engineering


- Features based on recent past experiences (12, 24, 48, 72, 168 hours)

- Min, max, and median response (all items)

- History (count) of past lapses (item 1) and completed EMAs (compliance)

- Raw scores and change scores (from baseline/all past responses)

::: {.notes}
We used these raw EMAs to engineer about 300 features to use in machine learning models to predict future lapses

We formed features by aggregating EMA items over various past time periods ranging from 12 -168 hours in the past

We calculated mins, maxes and medians for the EMA items in these time periods

We also calculated counts of past lapses and counts of past EMAs completed to index engagment with our monitoring system.

And we included these scores both in raw form and as change from baselines for the participant based on all their previous responses since the start of the study.
:::

## Machine Learning Methods


- Predict hour-by-hour probability of [future lapse]{.red}

- Lapse window widths
  - 1 hour
  - 1 day
  - 1 week

::: {.notes}

For our purposes today I wont dive deep into the machine learning methods but let me highlight a few high level details 

We used these features I just described to make predictions about the hour-by-hour probability of a future lapse.  We are developing separate models for three future lapse windows – lapses in the next hour, lapses in the next day, and lapses in the next week.  

For example, if I was in recovery from an AUD, I could use these models to generate the probability that I would lapse after this lecture starting at 1pm.  One model would generate the probability of a lapse between 1pm and 2pm today, the second would predict the probability of a lapse between 1pm today and 1pm tomorrow and the third would provide the probability of a lapse anytime between 1pm today and 1pm next Friday  

And of course, all of the models would only use data collected prior to 1pm today so that they are “predicting”, in the full sense of the word, into the future and not just demonstrating an association.

:::

## Machine Learning Methods {.medium}

- Statistical Algorithms
  - ElasticNet GLM (e.g., LASSO, ridge regression)
  - Random Forest
  - XGBoost
  - KNN

- Model Tuning and Performance Evaluation
  - Area under ROC curve (AUC) as primary performance metric
  - Sensitivity, Specificity, Balanced accuracy, Positive predictive value
  - Using [grouped]{.red} (by participant) [nested k-fold CV]{.red} 

::: {.notes}
We are evaluating machine learning model configurations that differ by common statistical algorithms. 

We are evaluating these models primarily using the area under the ROC curve but we also consider and report a variety of other common metrics.

And, of course, these performance metrics are calculated for new observations and new participants that the models have never seen and were not trained on by using grouped 10-fold cross-validation.

:::

## 1 Week: Probabilities for No Lapse and Lapse

:::: {.columns}

::: {.column width="60%"}

- Model predicts [probability]{.red} of lapse in next week for “[new]{.red}” observations in test set

- Can panel predictions by [Ground Truth]{.red} (i.e., true lapse vs. no lapse observations

- Want high probabilities to be high for true lapses and low for true no lapses
:::


::: {.column width="40%"}
<!--Lapse Probability by Ground Truth"-->
{{< embed ../../../notebooks/ema_figs_probability.qmd#fig-week-no_dec_thres >}}


:::
::::

::: {.notes}
Ok, lets start with the model that provides the coarsest level of temporal specificity – 1 week, and let me take a moment to make the predictions that this machine learning model provides more concrete for you

On the right, you are looking at histograms of the lapse probability predictions that the model makes for all the weeks for all the patients in the held out folds.   

I’ve paneled these histograms by whether a lapse did or did not happen in reality for each predicted week.  The top panel is for weeks with lapses and the bottom panel is for weeks with no lapses.

Ideally, you want the predicted probabilities to be very high for weeks when there was a lapse and very low for weeks when there was no lapse.    

And this is exactly what we see for the one week lapse window model
:::


## 1 Week: Probabilities for No Lapse and Lapse

:::: {.columns}

::: {.column width="60%"}

- [ Model predicts probability of lapse in next week for “new” observations in test set]{.gray}

- [Can panel predictions for GROUND TRUTH lapse and no lapse observations]{.gray}

- [Want high probabilities to be high for true lapses and low for true no lapses]{.gray}

- Need decision threshold for classification [P(Y = Lapse | X) > 0.50 or Youden's J]

:::


::: {.column width="40%"}
<!--Lapse Probability by Ground Truth-->
{{< embed ../../../notebooks/ema_figs_probability.qmd#fig-week >}}

:::

::::
::: {.notes}
Now to move from probabilities to actual categorical decisions – in other words, predicting a lapse or no lapse in some specific week, we need a decision threshold.   A probability of .50 is often used for this threshold, but as we will discuss later, there are times when we might want to choose other thresholds.   

But for now, I will use .5 such that the model will predict “lapse” for all weeks with probabilities > .5 and it will predict “no-lapse” for all weeks with probabilities < .5


:::

## Performance Metrics by Lapse Window Width

<!--TEMP CREATE TABLE IN SLIDE DECK-->
```{r}
#| tbl-cap: Performance Metrics by Model 

library(tidyverse, quietly = TRUE)
library(kableExtra, exclude = "group_rows", quietly = TRUE)

metrics <- read_csv(here::here("notebooks/analysis_objects/ema_metrics.csv"),
         col_types = cols()) |> 
  mutate(.metric = case_when(.metric == "roc_auc" ~ "auROC",
                        .metric == "sens" ~ "sensitivity",
                        .metric == "spec" ~ "specificity",
                        .metric == "bal_accuracy" ~ "balanced accuracy",
                        .metric == "ppv" ~ "ppv"))

metrics |> 
  slice(1:4) |>
  mutate(Day = "", Hour = "") |> 
  kbl(col.names = c("", "Week", "Day", "Hour"),
    digits = 2,
    align = c("r", "c", "c", "c"),
    linesep = "") |>
  row_spec(row = 0, align = "c") |>
  kable_styling(full_width = FALSE) |>
  kable_classic("striped") |>
  column_spec(2, color  = "red", bold = TRUE)
```

::: {.notes}
Using this decision threshold, we can now calculate the model’s sensitivity, specificity and balanced accuracy which is just the average of these two.

This one week lapse prediction model correctly predicts “lapse” for 79 percent of the weeks that contain a lapse and it correctly predicts “no-lapse” for 85 percent of the weeks that do not include a lapse.   




:::

## 1 Week: ROC Curve

:::: {.columns}

::: {.column width="50%"}

Area under the ROC curve (AUC)

 - Across all decision thresholds

 - ~.5 (random) – 1.0 (perfect)



::: {.absolute left="0%" bottom="0%" width="300px" height="auto"}
::: {style="font-size: 75%;"}
:::{.callout-tip icon=false}
## Coarse rules of thumb for AUC 
.70 - .80 are considered fair  
.80 - .90 are considered good  
\> .90 are considered excellent  
:::
:::
:::

:::


::: {.column width="50%"}

<!--ROC Curve for Week-->
{{< embed ../../../notebooks/ema_figs_roc.qmd#fig-roc-week >}}

:::

::::

::: {.notes}
But as I said, depending on the application, we may not want to always use a .5 decision threshold.  And this is where the ROC curve and the area under this curve come into play as a performance metric.   The ROC curve is a plot of the model’s sensitivity by its specificity across all possible decision thresholds.

The area under this curve can range from approximately .5 for a random classifier to 1.0 for a classifier that performs perfectly.  

And the AUC for our 1 week model is .90, which is generally considered excellent performance.  

:::

## 1 Day: ROC Curve


::: {.absolute left="0%" bottom="0%" width="300px" height="auto"}
::: {style="font-size: 75%;"}
:::{.callout-tip icon=false}
## Coarse rules of thumb for AUC 
.70 - .80 are considered fair  
.80 - .90 are considered good  
\> .90 are considered excellent  
:::
:::
:::


::: {.notes}
So we were very encouraged by the performance of this model because it exceeded the performance of the only other published week level lapse prediction model that we were aware of.   

But we were also eager to see how well we could do if we required a higher level of temporal precision by developing a day level model.



:::


## 1 Day: ROC Curve

:::: {.columns}

::: {.column width="50%"}

::: {.absolute left="0%" bottom="0%" width="300px" height="auto"}
::: {style="font-size: 75%;"}
:::{.callout-tip icon=false}
## Coarse rules of thumb for AUC 
.70 - .80 are considered fair  
.80 - .90 are considered good  
\> .90 are considered excellent  
:::
:::
:::


:::


::: {.column width="50%"}

<!--ROC Curve for Week & Day-->
{{< embed ../../../notebooks/ema_figs_roc.qmd#fig-roc-week_day >}}

:::
::::

::: {.notes}
And here is the ROC curve and the AUC for the day level model in green, superimposed on the week level model in orange

The day level model performed as well as, if not better, than the week level model with an AUC of .91

[PAUSE]





:::

## Performance Metrics by Lapse Window Width

<!--TEMP CREATE TABLE IN SLIDE DECK-->
```{r}
#| tbl-cap: Performance Metrics by Model 

metrics |> 
  slice(1:4) |>
  mutate(Hour = "") |>
  kbl(col.names = c("", "Week", "Day", "Hour"),
    digits = 2,
    align = c("r", "c", "c", "c"),
    linesep = "") |>
  row_spec(row = 0, align = "c") |>
  kable_styling(full_width = FALSE) |>
  kable_classic("striped") |>
  column_spec(3, color  = "red", bold = TRUE)
```

::: {.notes}
And consistent with this higher AUC, the day level model also had slightly better sensitivity and balanced accuracy, along with comparable specificity to the week level model

[PAUSE]


:::


## 1 Hour: ROC Curve


::: {.absolute left="0%" bottom="0%" width="300px" height="auto"}
::: {style="font-size: 75%;"}
:::{.callout-tip icon=false}
## Coarse rules of thumb for AUC 
.70 - .80 are considered fair  
.80 - .90 are considered good  
\> .90 are considered excellent  
:::
:::
:::


::: {.notes}
Given our success with this day level model, we next developed a model with the highest level of temporal precision we could build given how we measured lapses, which was the hour level model.  

:::




## 1 Hour: ROC Curve

:::: {.columns}

::: {.column width="50%"}

::: {.absolute left="0%" bottom="0%" width="300px" height="auto"}
::: {style="font-size: 75%;"}
:::{.callout-tip icon=false}
## Coarse rules of thumb for AUC 
.70 - .80 are considered fair  
.80 - .90 are considered good  
\> .90 are considered excellent  
:::
:::
:::


:::


::: {.column width="50%"}

<!--ROC Curve for All-->
{{< embed ../../../notebooks/ema_figs_roc.qmd#fig-roc-all >}}

:::
::::

::: {.notes}

And it turns out that we can do somewhat better still with hour level predictions.  Here the blur curve represents the ROC curve for the hour level model, which had the best AUC yet, .93

[PAUSE]


:::


## Performance Metrics by Lapse Window Width

<!--TEMP CREATE TABLE IN SLIDE DECK-->
```{r}
#| tbl-cap: Performance Metrics by Model 

metrics |> 
  slice(1:4) |>
  kbl(col.names = c("", "Week", "Day", "Hour"),
    digits = 2,
    align = c("r", "c", "c", "c"),
    linesep = "") |>
  row_spec(row = 0, align = "c") |>
  kable_styling(full_width = FALSE) |>
  kable_classic("striped") |>
  column_spec(4, color  = "red", bold = TRUE)
```

::: {.notes}
And again, consistent with that AUC, the sensitivity, specificity, and balanced accuracy were higher still for this model.

[PAUSE]

:::


## (Selective) Next Steps

- Geolocation - Claire FYP
- Cellular communications - Coco FYP


::: {.notes}
One of the key next steps is to develop models that rely more on passive sensing rather than EMA to lower the patient burden of using these systems long term and to gain access to signals that may not be available by self-report.

Let's take a look at two of the more revealing personal sensing methods that we are developing to provide you with some intuition about how we think this will work.

:::


## (Selective) Next Steps

- [Geolocation - Claire FYP]{.gray}
- [Cellular communications - Coco FYP]{.gray}

- Build models with lead times > 0 hours





::: {.notes}
NEED TEXT FOR THIS BUT ITS IMPORTANT.   FOCUS ON lead = 0 for 1 hour and JIT – distraction, urge surfing but lead = 1 week for contact with therapist, sponsor, supportive friends and family, etc.

:::


## (Selective) Next Steps


- [Geolocation - Claire FYP]{.gray}
- [Cellular communications - Coco FYP]{.gray}

- [Build models with lead times > 0 hours]{.gray}

- More diversity in training data

::: {.notes}
We are excited by the early performance of these machine learning models BUT I want to be clear that these are preliminary research studies.  And they included mostly white participants from our local community in Madison, WI.  

Models trained on these participants would be unlikely to work well with black and brown patients or patients from rural communities.  

Machine learning models must be trained on diverse samples of patients or their use may exacerbate rather than reduce existing mental healthcare disparities. 
:::

## Active Project: Lapse in patients with Opioid Use Disorder 

::: {.medium}
- Recruiting 400 patients in recovery from Opioid Use Disorder (All enrolled, many completed )
- National sample (size; diversity: demographics, location)
- More variation in stage of recover (1 – 6 months at start)
- 12 months of monitoring
- Closer to real implementation methods
:::
:::: {.columns}

::: {.column width="60%"}
![risk2_pis.png](https://github.com/jjcurtin/lectures/blob/main/images/risk2_pis.png?raw=true)\ 
:::
::: {.column width="40%"}
![nida_logo.png](https://github.com/jjcurtin/lectures/blob/main/images/nida_logo.png?raw=true)\ 
:::
::::
::: {.notes}
We are now collecting data for a NIDA funded project where we are specifically recruiting for racial, ethnic, and geographic diversity across the entire United States.


We are also recruiting for people at different stages in their recovery and following them for a longer period of time – up to 12 months

:::

## (Selective) Next Steps


- [Geolocation - Claire FYP]{.gray}
- [Cellular communications - Coco FYP]{.gray}

- [Build models with lead times > 0 hours]{.gray}

- [More diversity in training data]{.gray}

- Use models to improve DTx engagement and clinical outcomes
  - SMART DTx – algorithm guided use
  - How to craft patient feedback to encourage trust in the algorithm


::: {.notes}
Of course, accurately predicting future lapses is only useful if these predictions can be used to sustain engagement with treatments and improve clinical outcomes.  

We have now started to consider how we can use these predictions to help patients optimize their use of a digital therapeutic.  

These models may be able to suggest when more use of the DTx is needed because lapse risk is increasing or high but we are also hoping that we can use the models to recommend which tools and supports in the DTx are best for that patient at that moment in time given their lapse risk probability and important features contributing to the lapse prediction.

We are also sensitive to the fact that these recommendations from the machine learning models will need to be provided to patients in a transparent manner that also encourages them to trust the algorithm and follow its recommendations.  

:::

## Global Feature Importance by Model{.medium}

:::: {.columns}

::: {.column width="50%"}

- All EMA items impact lapse probability (both globally and locally)

:::

::: {.column width="50%"}

<!--Global SHAP Plot-->
{{< embed ../../../notebooks/ema_figs_shaps.qmd#fig-global >}}

:::

::::

::: {.notes}
In the spirit of making this model more transparent and interpretable, lets briefly look under the hood at the feature importance

The plot on the right shows feature names and their associated importance indexed by mean absolute SHAP values over all observations.  From this we see a few important characteristics of the model.   

First, all of EMA items affect predictions about lapse probability globally across observations  As you might expect, history of past lapses has a big influence on the probability of a future lapse.  But self reported abstinence efficacy, craving, history of stress events and other features from the EMA all make meaningful contributions to lapse probability across observations.  

To make these values somewhat meaningful for you, let me remind you these Sh
[logodds of .4 = 1.5 odds;  logodds of .7 = double the odds]

:::

## Local Importance

<!-- Local SHAP - Hour model-->
{{< embed ../../../notebooks/ema_figs_shaps.qmd#fig-local_hour >}}

<!-- Local SHAP - Day model-->
{{< embed ../../../notebooks/ema_figs_shaps.qmd#fig-local_day >}}

<!-- Local SHAP - Week model-->
{{< embed ../../../notebooks/ema_figs_shaps.qmd#fig-local_week >}}

::: {.notes}

:::

## Relapse Prevention Model

![relapse_prevention_flowchart.png](https://github.com/jjcurtin/lectures/blob/main/images/relapse_prevention_flowchart.png?raw=true){.absolute top ="20%" left="10%" width="73%" height="auto"}\ 


::: {.notes}

:::

## Optimization of an Algorithm Guided Smart DTx {.medium}

- Lapse probabilities updated daily based on EMA and Geolocation features
- Use lapse probability and locally important features to [recommend optimal DTx modules]{.red} – guided by Relapse Prevention model
- Provide recommendations with factorial manipulation of  [algorithmic transparency]{.red}
  - risk level 
  - risk change
  - key important features
- Outcomes
  - Engagement with recommended module
  - Heavy drinking days
  
![nida_logo.png](https://github.com/jjcurtin/lectures/blob/main/images/niaaa_logo.png?raw=true){.absolute bottom="5%" right="2%" width="auto" height="auto"}\ 

::: {.notes}

:::

## CRediTs

![credits.png](https://github.com/jjcurtin/lectures/blob/main/images/credits_risk1.png?raw=true){.absolute bottom="0%" left="5%" width="92%" height="auto"}\ 

::: {.notes}

:::

## Demographics & Alcohol Use History
<!-- JOHN looks like the latest version of the paper combined these tables - is that okay? --> 

<!-- Demographics table-->
```{r}
# footnote_table_dem_a <- "N = 151"
# footnote_table_dem_b <- "Two participants reported 100 or more quit attempts. We removed these outliers prior to calculating the mean (M), standard deviation (SD), and range."
```

```{r table_demo_auh}
# options(knitr.kable.NA = "")
# 
# dem <- screen %>% 
#   summarise(mean = as.character(round(mean(dem_1, na.rm = TRUE), 1)),
#             SD = as.character(round(sd(dem_1, na.rm = TRUE), 1)),
#             min = as.character(min(dem_1, na.rm = TRUE)),
#             max = as.character(max(dem_1, na.rm = TRUE))) %>% 
#   mutate(var = "Age",
#          n = as.numeric(""),
#          perc = as.numeric("")) %>% 
#   select(var, n, perc, everything()) %>% 
#   full_join(screen %>% 
#   select(var = dem_2) %>% 
#   group_by(var) %>% 
#   summarise(n = n()) %>% 
#   mutate(perc = (n / sum(n)) * 100), by = c("var", "n", "perc")) %>% 
#   full_join(screen %>% 
#   select(var = dem_3) %>% 
#   mutate(var = fct_relevel(factor(var, 
#                          c("American Indian/Alaska Native", "Asian", "Black/African American",
#                            "White/Caucasian", "Other/Multiracial")))) %>%
#   group_by(var) %>% 
#   summarise(n = n()) %>% 
#   mutate(perc = (n / sum(n)) * 100), by = c("var", "n", "perc")) %>% 
#   full_join(screen %>% 
#   select(var = dem_4) %>% 
#   mutate(var = case_when(var == "No, I am not of Hispanic, Latino, or Spanish origin" ~ "No",
#                          TRUE ~ "Yes"),
#          var = fct_relevel(factor(var, c("Yes", "No")))) %>% 
#   group_by(var) %>% 
#   summarise(n = n()) %>% 
#   mutate(perc = (n / sum(n)) * 100), by = c("var", "n", "perc")) %>% 
#   full_join(screen %>% 
#   select(var = dem_5) %>% 
#   mutate(var = fct_relevel(factor(var, 
#                          c("Less than high school or GED degree", "High school or GED", 
#                            "Some college", "2-Year degree", "College degree", "Advanced degree")))) %>%
#   group_by(var) %>% 
#   summarise(n = n()) %>% 
#   mutate(perc = (n / sum(n)) * 100), by = c("var", "n", "perc")) %>% 
#   full_join(screen %>% 
#   select(var = dem_6, dem_6_1) %>% 
#   mutate(var = case_when(dem_6_1 == "Full-time" ~ "Employed full-time",
#                          dem_6_1 == "Part-time" ~ "Employed part-time",
#                          TRUE ~ var)) %>% 
#   mutate(var = fct_relevel(factor(var, 
#                          c("Employed full-time", "Employed part-time", "Full-time student",
#                            "Homemaker", "Disabled", "Retired", "Unemployed", 
#                            "Temporarily laid off, sick leave, or maternity leave",
#                            "Other, not otherwise specified")))) %>%
#   group_by(var) %>% 
#   summarise(n = n()) %>% 
#   mutate(perc = (n / sum(n)) * 100), by = c("var", "n", "perc")) %>% 
#   full_join(screen %>% 
#   summarise(mean = format(round(mean(dem_7, na.rm = TRUE), 0), big.mark = ","),
#             SD = format(round(sd(dem_7, na.rm = TRUE), 0), big.mark = ","),
#             min =format(round(min(dem_7, na.rm = TRUE), 0), big.mark = ","),
#             max = format(round(max(dem_7, na.rm = TRUE), 0), scientific = FALSE, big.mark = ",")) %>% 
#   mutate(var = "Personal Income",
#         n = as.numeric(""),
#         perc = as.numeric(""),
#         mean = str_c("$", as.character(mean)),
#         SD = str_c("$", as.character(SD)),
#         min = str_c("$", as.character(min)),
#         max = as.character(max)) %>% 
#   select(var, n, perc, everything()), by = c("var", "n", "perc", "mean", "SD", "min", "max")) %>% 
#   full_join(screen %>% 
#   select(var = dem_8) %>% 
#   mutate(var = case_when(var == "Never Married" ~ "Never married",
#                          TRUE ~ var)) %>% 
#   mutate(var = fct_relevel(factor(var, 
#                          c("Never married", "Married", "Divorced", "Separated",
#                            "Widowed")))) %>%
#   group_by(var) %>% 
#   summarise(n = n()) %>% 
#   mutate(perc = (n / sum(n)) * 100), by = c("var", "n", "perc"))
# 
# auh <- screen %>% 
#   summarise(mean = mean(auh_1, na.rm = TRUE),
#             SD = sd(auh_1, na.rm = TRUE),
#             min = min(auh_1, na.rm = TRUE),
#             max = max(auh_1, na.rm = TRUE)) %>% 
#   mutate(var = "Age of first drink",
#         n = as.numeric(""),
#         perc = as.numeric("")) %>% 
#   select(var, n, perc, everything()) %>% 
#   full_join(screen %>% 
#   summarise(mean = mean(auh_2, na.rm = TRUE),
#             SD = sd(auh_2, na.rm = TRUE),
#             min = min(auh_2, na.rm = TRUE),
#             max = max(auh_2, na.rm = TRUE)) %>% 
#   mutate(var = "Age of regular drinking",
#         n = as.numeric(""),
#         perc = as.numeric("")) %>% 
#   select(var, n, perc, everything()), by = c("var", "n", "perc", "mean", "SD", 
#                                              "min", "max")) %>% 
#   full_join(screen %>% 
#   summarise(mean = mean(auh_3, na.rm = TRUE),
#             SD = sd(auh_3, na.rm = TRUE),
#             min = min(auh_3, na.rm = TRUE),
#             max = max(auh_3, na.rm = TRUE)) %>% 
#   mutate(var = "Age at which drinking became problematic",
#         n = as.numeric(""),
#         perc = as.numeric("")) %>% 
#   select(var, n, perc, everything()), by = c("var", "n", "perc", "mean", "SD",
#                                              "min", "max")) %>% 
#   full_join(screen %>% 
#   summarise(mean = mean(auh_4, na.rm = TRUE),
#             SD = sd(auh_4, na.rm = TRUE),
#             min = min(auh_4, na.rm = TRUE),
#             max = max(auh_4, na.rm = TRUE)) %>% 
#   mutate(var = "Age of first quit attempt",
#         n = as.numeric(""),
#         perc = as.numeric("")) %>% 
#   select(var, n, perc, everything()), by = c("var", "n", "perc", "mean", "SD",
#                                              "min", "max")) %>% 
#   full_join(screen %>% 
#   # filter out 2 people with 100 and 365 reported quit attempts - will make footnote in table
#   filter(auh_5 < 100) %>% 
#   summarise(mean = mean(auh_5, na.rm = TRUE),
#             SD = sd(auh_5, na.rm = TRUE),
#             min = min(auh_5, na.rm = TRUE),
#             max = max(auh_5, na.rm = TRUE)) %>% 
#   mutate(var = "Number of Quit Attempts*",
#         n = as.numeric(""),
#         perc = as.numeric("")) %>% 
#   select(var, n, perc, everything()), by = c("var", "n", "perc", "mean", "SD",
#                                              "min", "max")) %>% 
#   full_join(screen %>% 
#   select(var = auh_6_1) %>%
#   mutate(var = case_when(var == "Long-Term Residential Treatment (more than 6 months)" ~ "Long-term residential (6+ months)",
#                          TRUE ~ var)) %>% 
#   group_by(var) %>% 
#   drop_na() %>% 
#   summarise(n = n()) %>% 
#   mutate(perc = (n / 154) * 100), by = c("var", "n", "perc")) %>% 
#   full_join(screen %>% 
#   select(var = auh_6_2) %>%
#   mutate(var = case_when(var == "Short-Term Residential Treatment (less than 6 months)" ~ "Short-term residential (< 6 months)",
#                          TRUE ~ var)) %>% 
#   group_by(var) %>% 
#   drop_na() %>% 
#   summarise(n = n()) %>% 
#   mutate(perc = (n / 154) * 100), by = c("var", "n", "perc")) %>% 
#   full_join(screen %>% 
#   select(var = auh_6_3) %>%
#   mutate(var = case_when(var == "Outpatient Treatment" ~ "Outpatient",
#                          TRUE ~ var)) %>% 
#   group_by(var) %>% 
#   drop_na() %>% 
#   summarise(n = n()) %>% 
#   mutate(perc = (n / 154) * 100), by = c("var", "n", "perc")) %>% 
#   full_join(screen %>% 
#   select(var = auh_6_4) %>%
#   mutate(var = case_when(var == "Individual Counseling" ~ "Individual counseling",
#                          TRUE ~ var)) %>% 
#   group_by(var) %>% 
#   drop_na() %>% 
#   summarise(n = n()) %>% 
#   mutate(perc = (n / 154) * 100), by = c("var", "n", "perc")) %>% 
#   full_join(screen %>% 
#   select(var = auh_6_5) %>%
#   mutate(var = case_when(var == "Group Counseling" ~ "Group counseling",
#                          TRUE ~ var)) %>% 
#   group_by(var) %>% 
#   drop_na() %>% 
#   summarise(n = n()) %>% 
#   mutate(perc = (n / 154) * 100), by = c("var", "n", "perc")) %>% 
#   full_join(screen %>% 
#   select(var = auh_6_6) %>%
#   group_by(var) %>% 
#   drop_na() %>% 
#   summarise(n = n()) %>% 
#   mutate(perc = (n / 154) * 100), by = c("var", "n", "perc")) %>% 
#   full_join(screen %>% 
#   select(var = auh_6_7) %>%
#   group_by(var) %>% 
#   drop_na() %>% 
#   summarise(n = n()) %>% 
#   mutate(perc = (n / 154) * 100), by = c("var", "n", "perc")) %>% 
#   full_join(screen %>% 
#   select(var = auh_7) %>% 
#   mutate(var = fct_relevel(factor(var, c("Yes", "No")))) %>%
#   group_by(var) %>% 
#   summarise(n = n()) %>% 
#   mutate(perc = (n / sum(n)) * 100), by = c("var", "n", "perc")) %>% 
#   full_join(screen %>% 
#   mutate(across(dsm5_1:dsm5_11, ~ recode(., "No" = 0, "Yes" = 1))) %>% 
#   rowwise() %>% 
#   # calculate dsm5 score by adding up dsm5_1 through dsm5_11
#   mutate(dsm5_total = sum(c(dsm5_1, dsm5_2, dsm5_3, dsm5_4, dsm5_5, dsm5_6, dsm5_7, 
#                             dsm5_8, dsm5_9, dsm5_10, dsm5_11))) %>% 
#   ungroup() %>% 
#   summarise(mean = mean(dsm5_total),
#             SD = sd(dsm5_total),
#             min = min(dsm5_total, na.rm = TRUE),
#             max = max(dsm5_total, na.rm = TRUE)) %>% 
#   mutate(var = "Alcohol Use Disorder DSM-5 Symptom Count",
#         n = as.numeric(""),
#         perc = as.numeric("")) %>% 
#   select(var, n, perc, everything()), by = c("var", "n", "perc", "mean", "SD",
#                                              "min", "max")) %>% 
#   full_join(screen %>% 
#   select(var = assist_2_1) %>%
#   filter(var != "Never" & !is.na(var)) %>% 
#   mutate(var = "Tobacco products (cigarettes, chewing tobacco, cigars, etc.)") %>% 
#   group_by(var) %>% 
#   drop_na() %>% 
#   summarise(n = n()) %>% 
#   mutate(perc = (n / 154) * 100), by = c("var", "n", "perc")) %>% 
#   full_join(screen %>% 
#   select(var = assist_2_2) %>%
#   filter(var != "Never" & !is.na(var)) %>% 
#   mutate(var = "Cannabis (marijuana, pot, grass, hash, etc.)") %>% 
#   group_by(var) %>% 
#   drop_na() %>% 
#   summarise(n = n()) %>% 
#   mutate(perc = (n / 154) * 100), by = c("var", "n", "perc")) %>% 
#   full_join(screen %>% 
#   select(var = assist_2_3) %>%
#   filter(var != "Never" & !is.na(var)) %>% 
#   mutate(var = "Cocaine (coke, crack, etc.)") %>% 
#   group_by(var) %>% 
#   drop_na() %>% 
#   summarise(n = n()) %>% 
#   mutate(perc = (n / 154) * 100), by = c("var", "n", "perc")) %>% 
#   full_join(screen %>% 
#   select(var = assist_2_4) %>%
#   filter(var != "Never" & !is.na(var)) %>% 
#   mutate(var = "Amphetamine type stimulants (speed, diet pills, ecstasy, etc.)") %>% 
#   group_by(var) %>% 
#   drop_na() %>% 
#   summarise(n = n()) %>% 
#   mutate(perc = (n / 154) * 100), by = c("var", "n", "perc")) %>% 
#   full_join(screen %>% 
#   select(var = assist_2_5) %>%
#   filter(var != "Never" & !is.na(var)) %>% 
#   mutate(var = "Inhalants (nitrous, glue, petrol, paint thinner, etc.)") %>% 
#   group_by(var) %>% 
#   drop_na() %>% 
#   summarise(n = n()) %>% 
#   mutate(perc = (n / 154) * 100), by = c("var", "n", "perc")) %>% 
#   full_join(screen %>% 
#   select(var = assist_2_6) %>%
#   filter(var != "Never" & !is.na(var)) %>% 
#   mutate(var = "Sedatives or sleeping pills (Valium, Serepax, Rohypnol, etc.)") %>% 
#   group_by(var) %>% 
#   drop_na() %>% 
#   summarise(n = n()) %>% 
#   mutate(perc = (n / 154) * 100), by = c("var", "n", "perc")) %>% 
#   full_join(screen %>% 
#   select(var = assist_2_7) %>%
#   filter(var != "Never" & !is.na(var)) %>% 
#   mutate(var = "Hallucinogens (LSD, acid, mushrooms, PCP, Special K, etc.)") %>% 
#   group_by(var) %>% 
#   drop_na() %>% 
#   summarise(n = n()) %>% 
#   mutate(perc = (n / 154) * 100), by = c("var", "n", "perc")) %>% 
#   full_join(screen %>% 
#   select(var = assist_2_8) %>%
#   filter(var != "Never" & !is.na(var)) %>% 
#   mutate(var = "Opioids (heroin, morphine, methadone, codeine, etc.)") %>% 
#   group_by(var) %>% 
#   drop_na() %>% 
#   summarise(n = n()) %>% 
#   mutate(perc = (n / 154) * 100), by = c("var", "n", "perc")) 
```


```{r}
# display and format table
# dem %>% 
#   bind_rows(auh %>% 
#               mutate(across(mean:max, ~round(.x, 1))) %>% 
#               mutate(across(mean:max, ~as.character(.x)))) %>% 
#   mutate(range = str_c(min, "-", max)) %>%
#   select(-c(min, max)) %>% 
#   kbl(longtable = TRUE,
#       booktabs = TRUE,
#       col.names = c("", "N", "%", "M", "SD", "Range"),
#       align = c("l", "c", "c", "c", "c", "c"),
#       digits = 1,
#       caption = "Demographics and Alcohol Use Information") %>%
#   kable_styling(font_size = 20, full_width = FALSE) %>% 
#   kable_classic("striped") |>
#   row_spec(row = 0, align = "c", italic = TRUE) %>% 
#   pack_rows("Sex", 2, 3, bold = FALSE) %>% 
#   pack_rows("Race", 4, 8, bold = FALSE) %>% 
#   pack_rows("Hispanic, Latino, or Spanish Origin", 9, 10, bold = FALSE) %>% 
#   pack_rows("Education", 11, 16, bold = FALSE) %>% 
#   pack_rows("Employment", 17, 25, bold = FALSE) %>% 
#   pack_rows("Marital Status", 27, 31, bold = FALSE) %>% 
#   pack_rows("Alcohol Use Disorder Milestones", 32, 35, bold = FALSE) %>% 
#   pack_rows("Lifetime History of Treatment (Can choose more than 1)", 37, 43, bold = FALSE) %>% 
#   pack_rows("Received Medication for Alcohol Use Disorder", 44, 45, bold = FALSE) %>% 
#   pack_rows("Current (Past 3 Month) Drug Use", 47, 54, bold = FALSE) %>% 
#   footnote(general=footnote_table_dem_a, symbol = c(footnote_table_dem_b), 
#            threeparttable = TRUE)
```


## Consort Diagram

```{r load_consort_data}
# disposition <- read_csv(file.path(path_data_ema, "disposition.csv"), col_types = "ccDDcccccccccc")
```


```{r load_model_data}
# AUCs for 10x10 folds data
# auc_folds_week<- readRDS(file.path(path_models, "resample_metrics_best_all_1week_0_v4_kfold.rds"))
# auc_folds_day<- readRDS(file.path(path_models, "resample_metrics_best_all_1day_0_v4_kfold.rds"))
# auc_folds_hour<- readRDS(file.path(path_models, "resample_metrics_best_all_1hour_0_v4_kfold.rds")) 
# 
# # posterior probabilites
# pp <- readRDS(file.path(path_models, "posteriors_all_allwindows_0_v4_kfold.rds"))
# 

# # Predictions data
# preds_week<- readRDS(file.path(path_models, "resample_preds_best_all_1week_0_v4_kfold.rds"))
# preds_day<- readRDS(file.path(path_models, "resample_preds_best_all_1day_0_v4_kfold.rds"))
# preds_hour<- readRDS(file.path(path_models, "resample_preds_best_all_1hour_0_v4_kfold.rds")) 
# 
# # roc overall
# roc_week_full <- preds_week %>% 
#   roc_curve(prob, truth = truth)
# 
# roc_day_full <- preds_day %>% 
#   roc_curve(prob, truth = truth)
# 
# roc_hour_full <- preds_hour%>% 
#   roc_curve(prob, truth = truth)
# 
# # rocs per fold
# roc_week <- preds_week %>%
#   nest(.by = split_num, .key = "preds") %>% 
#   mutate(roc = map(preds, \(preds) roc_curve(preds, prob, 
#                                              truth = truth))) %>% 
#   mutate(model = "week")
# 
# roc_day <- preds_day %>%
#   nest(.by = split_num, .key = "preds") %>% 
#   mutate(roc = map(preds, \(preds) roc_curve(preds, prob, 
#                                              truth = truth))) %>% 
#   mutate(model = "day")
# 
# roc_hour <- preds_hour %>%
#   nest(.by = split_num, .key = "preds") %>% 
#   mutate(roc = map(preds, \(preds) roc_curve(preds, prob, 
#                                              truth = truth))) %>% 
#   mutate(model = "week")
# 
# # pr overall
# pr_week_full <- preds_week %>% 
#   pr_curve(prob, truth = truth)
# 
# pr_day_full <- preds_day %>% 
#   pr_curve(prob, truth = truth)
# 
# pr_hour_full <- preds_hour%>% 
#   pr_curve(prob, truth = truth)
# 
# # prs per fold
# pr_week <- preds_week %>% 
#   pr_curve(prob, truth = truth) %>% 
#   mutate(model = "1week")
# 
# pr_day <- preds_day %>% 
#   pr_curve(prob, truth = truth) %>% 
#   mutate(model = "1day")
# 
# pr_hour <- preds_hour%>% 
#   pr_curve(prob, truth = truth) %>% 
#   mutate(model = "1hour")
# 
# pr_all <- pr_week %>% 
#   bind_rows(pr_day) %>% 
#   bind_rows(pr_hour)
# 
# #raw SHAPs
# shap_raw_week <- readRDS(file.path(path_models, "imp_shap_raw_all_1week_0_v4.rds")) %>% 
#   group_by(variable) %>% 
#   slice(1) %>%   
#   ungroup() %>% 
#   arrange(mean_value)
# shap_raw_day <- readRDS(file.path(path_models, "imp_shap_raw_all_1day_0_v4.rds")) %>% 
#   group_by(variable) %>% 
#   slice(1) %>%   
#   ungroup() %>% 
#   arrange(mean_value)
# shap_raw_hour <- readRDS(file.path(path_models, "imp_shap_raw_all_1hour_0_v4.rds")) %>% 
#   group_by(variable) %>% 
#   slice(1) %>%   
#   ungroup() %>% 
#   arrange(mean_value)
# 
# # Grouped SHAPS
# shap_grouped_week <- readRDS(file.path(path_models, "imp_shap_grouped_all_1week_0_v4.rds")) 
# shap_grouped_day <- readRDS(file.path(path_models, "imp_shap_grouped_all_1day_0_v4.rds"))
# shap_grouped_hour <- readRDS(file.path(path_models, "imp_shap_grouped_all_1hour_0_v4.rds")) 
# 
# # lapse labels
# labels_week <- read_csv(file.path(path_data_ema, "labels_1week.csv"), col_types = cols())
# labels_day <- read_csv(file.path(path_data_ema, "labels_1day.csv"), col_types = cols())
# labels_hour <- read_csv(file.path(path_data_ema, "labels_1hour.csv"), col_types = cols())
```

<!-- Consort Diagram-->

```{r caption_consort}
fig_caption_consort <- "Consort Diagram"
```



```{r fig_consort}
#| fig.cap: !expr fig_caption_consort
#| fig.height: 7
# consort_plot(data = disposition,
#              orders = c(eligible = "Eligible Sample",
#                         consented_reason = "Not Consented",
#                         consented = "Consented",
#                         enrolled_reason = "Not Enrolled",
#                         enrolled = "Enrolled",
#                         completed_followup_reason = "Discontinued",
#                         completed_followup = "Completed through Followup 1",
#                         analysis_reason = "Excluded",
#                         analysis = "Final Analysis"),
#              side_box = c("consented_reason", 
#                           "enrolled_reason", 
#                           "completed_followup_reason",
#                           "analysis_reason"),
#              cex = .9,
#              text_width = 45)
```

## ROC Posterior Probabilities

<!-- AUC figure by model w/posterior probabilities-->  

```{r fig_roc_pp}
#| fig.height: 4.5  
#| fig.width: 7  
#| fig-align: "center"


# roc_plot <- roc_all %>% 
#   mutate(model = factor(model, levels = c("1week", "1day", "1hour"), 
#                         labels = c("Week", "Day", "Hour"))) %>% 
#   ggplot(aes(x = 1 - specificity, y = sensitivity, color = model)) +
#   geom_path(show.legend = FALSE) +
#   geom_abline(lty = 3) +
#   coord_fixed(xlim = c(0, 1), ylim = c(0, 1)) +
#   labs(x = "Specificity",
#        y = "Sensitivity") +
#   scale_x_continuous(breaks = seq(0,1,.25),
#     labels = sprintf("%.2f", seq(1,0,-.25))) 
# 
# pp_tidy <- pp %>% 
#   tidy(seed = 123)
# 
# ci <- pp_tidy %>% 
#   summary() %>% 
#   mutate(model = factor(model, levels = c("week", "day", "hour"),
#                         labels = c("Week", "Day", "Hour")),
#          y = 1000)
# 
# pp_plot <- pp_tidy %>% 
#   mutate(model = factor(model, levels = c("week", "day", "hour"),
#                         labels = c("Week", "Day", "Hour"))) %>%
#   ggplot() + 
#   geom_histogram(aes(x = posterior, fill = model), color = "black", alpha = .4, 
#                  bins = 30) +
#   geom_segment(mapping = aes(y = y+100, yend = y-100, x = mean, xend = mean,
#                            color = model),
#                show.legend = FALSE,
#                data = ci) +
#   geom_segment(mapping = aes(y = y, yend = y, x = lower, xend = upper, color = model),
#               show.legend = FALSE,
#                data = ci) +
#   geom_text(data = ci, x = c(.93, .907, .92), y = 1000, 
#             label = str_c(round(ci$mean, 2), " [", round(ci$lower, 2), ", ", round(ci$upper, 2), "]")) +
#   facet_wrap(~model, ncol = 1) +
#   scale_y_continuous("Posterior Probability", breaks = c(0, 500, 1000)) +
#   xlab("Area Under ROC Curve") +
#   theme(strip.background = element_blank(),
#         strip.text.x = element_blank())
# 
# roc_plot + pp_plot +
#   plot_layout (guides = "collect") &
#   theme(legend.position = "bottom")
```

## Model Comparison Posterior Probabilites



```{r caption_pp_contrasts}
fig_caption_pp_contrasts <- "Posterior Probabilities for Model Contrasts for AUC.   Region of Practical Equivalence (ROPE) indicated by dashed yellow lines"
```

```{r fig_posterior_d}
#| fig-cap: !expr fig_caption_pp_contrasts
#| fig.height:  7

# ci <- pp %>% 
#   contrast_models(list("hour","hour", "day"), 
#                 list("week", "day", "week")) %>% 
#   summary(size = .01) %>% 
#   mutate(contrast = factor(contrast, 
#                            levels = c("hour vs week", "hour vs day", "day vs week"),
#                            labels = c("Hour vs. Week", "Hour vs. Day", "Day vs. Week")),
#          y = 700)
# 
# pp %>% 
#   tidy(seed = 123) %>%   
#   group_by(model) %>% 
#   mutate(sample = row_number()) %>% 
#   ungroup() %>% 
#   pivot_wider(names_from = model, values_from = posterior) %>% 
#   mutate(hour_vs_week = hour - week,
#          hour_vs_day = hour - day,
#          day_vs_week = day - week) %>% 
#   pivot_longer(cols = hour_vs_week:day_vs_week,
#                names_to = "contrast",
#                values_to = "posterior") %>% 
#   mutate(contrast = factor(contrast, 
#                            levels = c("hour_vs_week", "hour_vs_day", "day_vs_week"),
#                            labels = c("Hour vs. Week", "Hour vs. Day", "Day vs. Week"))) %>% 
#   ggplot() +
#   geom_histogram(aes(x = posterior, fill = contrast), 
#                  color = "black", alpha = .4, bins = 30) +
#   geom_vline(xintercept = -.01, color = "yellow", linetype = "dashed", size = 1) +
#   geom_vline(xintercept = .01, color = "yellow", linetype = "dashed", size = 1) +
#   geom_segment(mapping = aes(y = y+100, yend = y-100, x = mean, xend = mean,
#                              color = contrast), data = ci, show.legend = FALSE) +
#   geom_segment(mapping = aes(y = y, yend = y, x = lower, xend = upper, 
#                              color = contrast), data = ci, show.legend = FALSE) +
#   geom_text(data = ci, x = c(.0255, .035, .015), y = 700, 
#             label = str_c(round(ci$mean, 2), " [", round(ci$lower, 2), ", ", round(ci$upper, 2), "]")) +
#   facet_wrap(~contrast, ncol = 1) +
#   xlab("Posterior")
  
```
